{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quanvu0996/TF_cert_training/blob/main/v2.5_ML20M%20optimizing%20cluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZQDJKGUAQqn",
        "outputId": "11d35c6b-bc1e-4d9f-a033-fbf00b06c3b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52.0\n",
            "svmem(total=13617745920, available=8679403520, percent=36.3, used=10406965248, free=772472832, active=5643710464, inactive=6951673856, buffers=49086464, cached=2389221376, shared=1257472, slab=146001920)\n",
            "36.3\n",
            "63.735977826204\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "# gives a single float value\n",
        "print(psutil.cpu_percent())\n",
        "# gives an object with many fields\n",
        "print(psutil.virtual_memory())\n",
        "# you can convert that object to a dictionary print(dict(psutil.virtual_memory()._asdict()))\n",
        "# you can have the percentage of used RAM\n",
        "print(psutil.virtual_memory().percent)\n",
        "# you can calculate percentage of available memory\n",
        "print(psutil.virtual_memory().available * 100 / psutil.virtual_memory().total)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons\n",
        "!pip install recommenders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7sPzaTKcdTo",
        "outputId": "a4204b69-fc24-4608-8324-b30672379128"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: recommenders in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: pandas<2,>1.0.3 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.5)\n",
            "Requirement already satisfied: jinja2<3.1,>=2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.11.3)\n",
            "Requirement already satisfied: transformers<5,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (4.19.2)\n",
            "Requirement already satisfied: lightfm<2,>=1.15 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.16)\n",
            "Requirement already satisfied: seaborn<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.11.2)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.3)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.21.6)\n",
            "Requirement already satisfied: pyyaml<6,>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (5.4.1)\n",
            "Requirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.4.1)\n",
            "Requirement already satisfied: tqdm<5,>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (4.64.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.23.0)\n",
            "Requirement already satisfied: nltk<4,>=3.4 in /usr/local/lib/python3.7/dist-packages (from recommenders) (3.7)\n",
            "Requirement already satisfied: numba<1,>=0.38.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.51.2)\n",
            "Requirement already satisfied: scikit-surprise>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.1.1)\n",
            "Requirement already satisfied: category-encoders<2,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.0)\n",
            "Requirement already satisfied: cornac<2,>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.14.2)\n",
            "Requirement already satisfied: memory-profiler<1,>=0.54.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn<1.0.3,>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.0.2)\n",
            "Requirement already satisfied: pandera[strategies]>=0.6.5 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.9.0)\n",
            "Requirement already satisfied: bottleneck<2,>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.4)\n",
            "Requirement already satisfied: lightgbm>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.2.3)\n",
            "Requirement already satisfied: matplotlib<4,>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (3.2.2)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.10.2)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.5.2)\n",
            "Requirement already satisfied: powerlaw in /usr/local/lib/python3.7/dist-packages (from cornac<2,>=1.1.2->recommenders) (1.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3.1,>=2->recommenders) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4,>=2.2.2->recommenders) (4.2.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler<1,>=0.54.0->recommenders) (5.4.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (0.34.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>1.0.3->recommenders) (2022.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (21.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (1.9.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (6.0.1)\n",
            "Requirement already satisfied: typing-inspect>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (0.7.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (1.14.1)\n",
            "Requirement already satisfied: hypothesis>=5.41.1 in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (6.46.11)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (21.4.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (2.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->category-encoders<2,>=1.3.0->recommenders) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.0.3,>=0.22.1->recommenders) (3.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (0.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (4.11.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (3.7.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from typing-inspect>=0.6.0->pandera[strategies]>=0.6.5->recommenders) (0.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5,>=2.5.0->recommenders) (3.8.0)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from powerlaw->cornac<2,>=1.1.2->recommenders) (1.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libs"
      ],
      "metadata": {
        "id": "oGKXb_a2GGDk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "XS6UHXPVAQqs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "import gc\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.losses import TripletSemiHardLoss \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from recommenders.datasets.python_splitters import python_random_split\n",
        "from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
        "from recommenders.models.cornac.cornac_utils import predict_ranking"
      ],
      "metadata": {
        "id": "0Sw1qgh7MJo0"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Gv8gXemwD3UQ",
        "outputId": "9336c7e9-f692-4f49-c7fc-da30f1d4177d"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "Crhty6fiAQqu"
      },
      "outputs": [],
      "source": [
        "itemCol = 'movieId'\n",
        "userCol = 'userId'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DGX setup\n",
        "# fpath = \"./ml-20m\" \n",
        "\n",
        "#colab setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "fpath = \"/content/gdrive/MyDrive/RECOMMENDER_STUDIES/data/ml-20m\""
      ],
      "metadata": {
        "id": "xOJJy441A0w0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350045d2-b33c-49e0-be28-cfe8cf0ae04a"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data\n",
        "ratings = pd.read_csv(fpath+'/ratings.csv')\n",
        "\n",
        "ratings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "jQck8YHxCPJ1",
        "outputId": "827da54b-ad1a-437e-c981-59bf44e51933"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          userId  movieId  rating   timestamp\n",
              "0              1        2     3.5  1112486027\n",
              "1              1       29     3.5  1112484676\n",
              "2              1       32     3.5  1112484819\n",
              "3              1       47     3.5  1112484727\n",
              "4              1       50     3.5  1112484580\n",
              "...          ...      ...     ...         ...\n",
              "20000258  138493    68954     4.5  1258126920\n",
              "20000259  138493    69526     4.5  1259865108\n",
              "20000260  138493    69644     3.0  1260209457\n",
              "20000261  138493    70286     5.0  1258126944\n",
              "20000262  138493    71619     2.5  1255811136\n",
              "\n",
              "[20000263 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6d6b4205-365e-4889-957e-b3983cc4cc85\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112486027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000258</th>\n",
              "      <td>138493</td>\n",
              "      <td>68954</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1258126920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000259</th>\n",
              "      <td>138493</td>\n",
              "      <td>69526</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1259865108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000260</th>\n",
              "      <td>138493</td>\n",
              "      <td>69644</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1260209457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000261</th>\n",
              "      <td>138493</td>\n",
              "      <td>70286</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1258126944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000262</th>\n",
              "      <td>138493</td>\n",
              "      <td>71619</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1255811136</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000263 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d6b4205-365e-4889-957e-b3983cc4cc85')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6d6b4205-365e-4889-957e-b3983cc4cc85 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6d6b4205-365e-4889-957e-b3983cc4cc85');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize rating về dạng -1 -> 1 (-1 = ghét, 1 = thích)\n",
        "ratings[\"y\"] = ratings[\"rating\"]/2.5-1\n",
        "\n",
        "# Kiểm tra rating trong khoảng -1 -> 1\n",
        "ratings.groupby(\"y\")[\"y\"].count().plot(kind='bar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "BrwYHetxCiIw",
        "outputId": "26450d04-5eb7-4235-d38a-703f92d76086"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fec64faaf50>"
            ]
          },
          "metadata": {},
          "execution_count": 110
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading movie\n",
        "movies = pd.read_csv(fpath+'/movies.csv')\n",
        "movies[\"year\"]=movies[\"title\"].apply(lambda x: x[-5:-1])\n",
        "movies[\"genres\"] = movies[\"genres\"].apply(lambda x: ' ' if x == '(no genres listed)' else ' '.join(x.split('|')) )\n",
        "movies[\"title\"]= movies[\"title\"].apply(lambda x: x[0:-7])\n",
        "movies.head()"
      ],
      "metadata": {
        "id": "_DH8SxPH64nZ",
        "outputId": "822de6fe-0613-4e7d-c9cc-64e7521c4ed6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   movieId                        title  \\\n",
              "0        1                    Toy Story   \n",
              "1        2                      Jumanji   \n",
              "2        3             Grumpier Old Men   \n",
              "3        4            Waiting to Exhale   \n",
              "4        5  Father of the Bride Part II   \n",
              "\n",
              "                                        genres  year  \n",
              "0  Adventure Animation Children Comedy Fantasy  1995  \n",
              "1                   Adventure Children Fantasy  1995  \n",
              "2                               Comedy Romance  1995  \n",
              "3                         Comedy Drama Romance  1995  \n",
              "4                                       Comedy  1995  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d20e0a0-afe3-4932-91ff-ed766bd4b349\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movieId</th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Toy Story</td>\n",
              "      <td>Adventure Animation Children Comedy Fantasy</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Jumanji</td>\n",
              "      <td>Adventure Children Fantasy</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Grumpier Old Men</td>\n",
              "      <td>Comedy Romance</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Waiting to Exhale</td>\n",
              "      <td>Comedy Drama Romance</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Father of the Bride Part II</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d20e0a0-afe3-4932-91ff-ed766bd4b349')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0d20e0a0-afe3-4932-91ff-ed766bd4b349 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0d20e0a0-afe3-4932-91ff-ed766bd4b349');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model v2.3\n",
        "Sử dụng constrastive loss thay cho onehot"
      ],
      "metadata": {
        "id": "HfA78F8HRVJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chia dữ liệu thành các tập cho model clustering, nhãn recommendation và test\n",
        "#     warm_up_mask: ratings đã quan sát\n",
        "#     target: rating dùng để đánh giá kết quả recommend của module recommend cho từng người dùng\n",
        "#     test: đánh giá độc lập\n",
        "# TODO: chia tập dữ liệu theo user-wise => đánh giá với những user hoàn toàn mới thì model có học được không?\n",
        "# v2.1. gộp warm_up và mask thành 1\n",
        "def dataset_split(ratings):\n",
        "    train, test = train_test_split(ratings, test_size= 0.25)\n",
        "    warm_up_mask, target = train_test_split(train, test_size= 0.25)\n",
        "    return warm_up_mask, target, test\n",
        "\n",
        "# warm_up_mask, target, test = dataset_split(ratings)\n",
        "warm_up_mask, test= train_test_split(ratings, test_size= 0.25)"
      ],
      "metadata": {
        "id": "90sKne1ZRc5F"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kiểm tra số lượng ratings\n",
        "warm_up_mask.shape[0], test.shape[0] #, target.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiRtTNy1ScIa",
        "outputId": "b00e8c89-43ea-486c-ac8e-0ba2bf9291dc"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000197, 5000066)"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# v2.2: chỉ groupby, không padding\n",
        "def get_interaction_set(interaction, max_item = None, top_k_item = None):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        interaction: df[userCol, itemCol, y]: dữ liệu đầu vào\n",
        "        max_item: int: item num limit\n",
        "    Output:\n",
        "        df, itemCol: list, y: list, itemCol_str: string, userCol as index\n",
        "        list item sắp xếp theo giảm dần độ lớn rating\n",
        "    \"\"\"\n",
        "    items = ratings.groupby(itemCol).count().sort_values(by=userCol, ascending=False)\n",
        "    if top_k_item is not None:\n",
        "        top_items = items.head(top_k_item).index\n",
        "        interaction = interaction[interaction[itemCol].isin(top_items)]\n",
        "    else:\n",
        "        top_items = items.index\n",
        "\n",
        "    # Sắp xếp item theo thứ tự giảm dần rating (về sau cắt padding sẽ ưu tiên giữ lại item có rating cao)\n",
        "    rindex = interaction.groupby(userCol)[\"y\"].transform(lambda grp: grp.sort_values(ascending=False).index)\n",
        "    interaction = interaction.reindex(rindex)\n",
        "    \n",
        "    # Chuyển thành warm-up set theo từng user\n",
        "    interaction = interaction.groupby(\"userId\").agg({itemCol:list, \"y\":list})\n",
        "\n",
        "    # Giới hạn độ dài warm_up size\n",
        "    if max_item is not None:\n",
        "        interaction[itemCol] = interaction[itemCol].apply(lambda x: x[0:max_item])\n",
        "        interaction[\"y\"] = interaction[\"y\"].apply(lambda x: x[0:max_item])\n",
        "\n",
        "    return interaction, top_items\n",
        "\n",
        "top_k_item = 20000\n",
        "wu_size = 200\n",
        "mask_size = 200\n",
        "max_item = wu_size + mask_size\n",
        "\n",
        "# interac_df, top_items = get_interaction_set(warm_up_mask[warm_up_mask[userCol]<3000]\n",
        "#                     , max_item = max_item\n",
        "#                     , top_k_item = top_k_item )\n",
        "# interac_df\n",
        "\n",
        "# define top_items\n",
        "top_items = ratings.groupby(itemCol).count().sort_values(by=userCol, ascending=False).head(top_k_item).index"
      ],
      "metadata": {
        "id": "C3ukIA2SS00l"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(top_items)"
      ],
      "metadata": {
        "id": "Oo-X6mlUn-CP"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class  model"
      ],
      "metadata": {
        "id": "HIfY6FeqKqQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo cấu hình: xác định encoder, decoder, mapping dims\n",
        "class Efficient_Rec(tf.keras.Model):\n",
        "  def __init__(self, encoder, decoder= None, reps=None, use_tf_function=False):\n",
        "    super().__init__()\n",
        "    self.use_tf_function = use_tf_function\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.reps = reps\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)"
      ],
      "metadata": {
        "id": "XtrIkMaRbo6v"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _wu_mask_split(self, batch_inputs, mask_ratio = 0.25):\n",
        "    \"Chia ratings và items thành mask, warm up\"\n",
        "    input_items, input_ratings = batch_inputs[itemCol], batch_inputs[\"y\"]\n",
        "\n",
        "    def list_split(input, mask_ratio = 0.25, seed= 42):\n",
        "        return train_test_split(input[0:(wu_size+mask_size)], test_size= mask_ratio, random_state=seed)\n",
        "\n",
        "    seed = random.randint(1, 100)\n",
        "    items = input_items.apply(lambda x: list_split(x, mask_ratio, seed))\n",
        "    ratings = input_ratings.apply(lambda x: list_split(x, mask_ratio, seed))\n",
        "\n",
        "    wu_item_list = items.apply(lambda x: x[0])\n",
        "    mask_item_list = items.apply(lambda x: x[1])\n",
        "\n",
        "    wu_rating_list = ratings.apply(lambda x: x[0])\n",
        "    mask_rating_list = ratings.apply(lambda x: x[1])\n",
        "    return (wu_item_list, wu_rating_list), (mask_item_list, mask_rating_list)\n",
        "\n",
        "Efficient_Rec._wu_mask_split = _wu_mask_split"
      ],
      "metadata": {
        "id": "mZtGCHBnjcCg"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _wu_mask_split2(self, batch_inputs, mask_ratio = 0.25):\n",
        "    \"Chia ratings và items thành mask, warm up, giữ nguyên full set trong warmup\"\n",
        "    input_items, input_ratings = batch_inputs[itemCol], batch_inputs[\"y\"]\n",
        "\n",
        "    def list_split(input, mask_ratio = 0.25, seed= 42):\n",
        "        return train_test_split(input[0:(wu_size+mask_size)], test_size= mask_ratio, random_state=seed)\n",
        "\n",
        "    seed = random.randint(1, 100)\n",
        "    items = input_items.apply(lambda x: list_split(x, mask_ratio, seed))\n",
        "    ratings = input_ratings.apply(lambda x: list_split(x, mask_ratio, seed))\n",
        "\n",
        "    wu_item_list = input_items #items.apply(lambda x: x[0])\n",
        "    mask_item_list = items.apply(lambda x: x[1])\n",
        "\n",
        "    wu_rating_list = input_ratings #ratings.apply(lambda x: x[0])\n",
        "    mask_rating_list = ratings.apply(lambda x: x[1])\n",
        "    return (wu_item_list, wu_rating_list), (mask_item_list, mask_rating_list)\n",
        "\n",
        "Efficient_Rec._wu_mask_split2 = _wu_mask_split2"
      ],
      "metadata": {
        "id": "38n-keEMtBQ7"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _preprocess(self, inputs, padding_size = 100):\n",
        "    \"\"\"\n",
        "    Padding về wu_size và mask_size, convert list of items => string of items\n",
        "    batch_inputs: df: itemStr, y\"\"\"\n",
        "\n",
        "    def padding_list(list_item, wu_size, value=0, is_padding=True):\n",
        "        series_item1 = list_item[0:wu_size]\n",
        "        if is_padding:\n",
        "            series_item1 = series_item1+[value]*(wu_size-len(series_item1))\n",
        "        return series_item1\n",
        "\n",
        "    items_list, ratings_list = inputs\n",
        "\n",
        "    items   = items_list.apply(lambda x: ' '.join(list([str(i) for i in x])))\n",
        "    ratings =   np.stack( ratings_list.apply(lambda x: padding_list( x, padding_size  ) ) )\n",
        "\n",
        "    return items, ratings\n",
        "\n",
        "Efficient_Rec._preprocess = _preprocess"
      ],
      "metadata": {
        "id": "OvIJP8m7bvaF"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện training\n",
        "def _train_step(self, inputs):\n",
        "    warm_up, mask = self._wu_mask_split(inputs, mask_ratio = 0.25)\n",
        "    wu_items, wu_ratings = self._preprocess(warm_up, wu_size)\n",
        "    mask_item, mask_ratings = self._preprocess(mask, mask_size)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Encode the input\n",
        "        wu_vec = self.encoder([wu_items, wu_ratings])\n",
        "        y_pred = self.reps(wu_vec)\n",
        "\n",
        "        # Encde the output\n",
        "        mask_vec = self.decoder([mask_item, mask_ratings])\n",
        "\n",
        "        average_loss = self.loss(mask_vec, y_pred)\n",
        "\n",
        "    # Apply an optimization step\n",
        "    variables = self.trainable_variables \n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "\n",
        "Efficient_Rec._train_step = _train_step"
      ],
      "metadata": {
        "id": "gzoNOhtuhHlu"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện minibatch training\n",
        "def _batch_train_step(self, inputs, batch_size):\n",
        "    df = inputs.copy()\n",
        "    chunks = [df[i:i+batch_size] for i in range(0,df.shape[0],batch_size)]\n",
        "    for chunk in chunks:\n",
        "        self._train_step(chunk)\n",
        "\n",
        "Efficient_Rec._batch_train_step = _batch_train_step"
      ],
      "metadata": {
        "id": "HxyczAa_cLdY"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện training\n",
        "def _constrastive_train_step(self, inputs):\n",
        "    warm_up, mask = self._wu_mask_split2(inputs.sample(frac=1), mask_ratio = 0.5)\n",
        "    wu_items, wu_ratings = self._preprocess(warm_up, wu_size)\n",
        "    mask_item, mask_ratings = self._preprocess(mask, wu_size)\n",
        "\n",
        "    # negative_items = wu_items.shift(1, axis=0, fill_value=['0 '])\n",
        "    negative_items = np.roll( wu_items.values, shift= 1, axis=0)\n",
        "    negative_items = pd.Series(negative_items)\n",
        "    negative_ratings =  np.roll( wu_ratings, shift= 1, axis=0)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Interaction embedding\n",
        "        wu_vec = self.encoder([wu_items, wu_ratings])\n",
        "        mask_vec = self.encoder([mask_item, mask_ratings])\n",
        "        negative_vec = self.encoder([negative_items, negative_ratings])\n",
        "\n",
        "        average_loss = self.loss(wu_vec, mask_vec, negative_vec) #- 0.1*np.mean([tf.linalg.norm(x) for x in self.trainable_variables])\n",
        "\n",
        "    # Apply an optimization step\n",
        "    variables = self.trainable_variables \n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    gradients = [None if gradient is None else tf.clip_by_value(gradient, -0.1, 0.1)\n",
        "                 for gradient in gradients]\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "\n",
        "Efficient_Rec._constrastive_train_step = _constrastive_train_step"
      ],
      "metadata": {
        "id": "vuBMn3DD0BaM"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện minibatch training\n",
        "# @tf.function\n",
        "def _constrastive_train_minibatch_step(self, inputs, batch_size):\n",
        "    df = inputs.copy()\n",
        "    chunks = [df[i:i+batch_size] for i in range(0,df.shape[0],batch_size)]\n",
        "    losses = []\n",
        "    for chunk in chunks:\n",
        "        loss = self._constrastive_train_step(chunk)\n",
        "        losses.append(loss[\"batch_loss\"].numpy())\n",
        "        print(loss)\n",
        "    return np.mean(losses)\n",
        "\n",
        "Efficient_Rec._constrastive_train_minibatch_step = _constrastive_train_minibatch_step"
      ],
      "metadata": {
        "id": "8DG-5bLRqIxc"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# staticmethod\n",
        "def get_top_cluster(scores, interaction_list):\n",
        "    interaction_list_ = interaction_list.copy()\n",
        "    idx = np.argsort(-scores.transpose(),axis=0)[:cluster_num]\n",
        "    interaction_list_[\"clusters\"] = [list(i) for i in idx.transpose()]\n",
        "    interaction_list_[\"scores\"] = [ list(scores[i][ind]) for i, ind in enumerate(idx.transpose()) ]\n",
        "    return interaction_list_\n",
        "\n",
        "def minibatch_clustering(self, interaction_list, batch_size= 512):\n",
        "    chunks = [interaction_list[i:i+batch_size] for i in range(0,interaction_list.shape[0],batch_size)]\n",
        "    preds = []\n",
        "    for chunk in chunks:\n",
        "        pred = self.encoder(self._preprocess( [chunk[itemCol], chunk[\"y\"]], padding_size = wu_size )).numpy()\n",
        "        preds.append( pred )\n",
        "\n",
        "    return np.concatenate(preds)\n",
        "\n",
        "def chunk_explode(ratings, batch_size = 1024**2):\n",
        "    chunks = [ratings[i:i+batch_size] for i in range(0,ratings.shape[0],batch_size)]\n",
        "    explodes = []\n",
        "\n",
        "    # Todo: convert for loop to parallel\n",
        "    for chunk in chunks:\n",
        "        explode = chunk.explode([\"clusters\", \"scores\"])\n",
        "        explode[\"contribute_score\"] = explode[\"scores\"].astype(\"float64\")*explode[\"y\"]\n",
        "        explode = explode.groupby([\"clusters\", \"movieId\"]).agg({\n",
        "            \"contribute_score\": [\"mean\", \"count\"]\n",
        "        }).reset_index()\n",
        "        explode.columns = [\"clusters\", \"movieId\", \"mean\", \"count\"]\n",
        "        explodes.append(explode)\n",
        "\n",
        "    # combine results\n",
        "    gr = pd.concat(explodes, axis = 0)\n",
        "    gr[\"product\"] = gr[\"mean\"]*gr[\"count\"]\n",
        "    gr = gr.groupby([\"clusters\", \"movieId\"]).sum().reset_index()\n",
        "    gr[\"contribute_score\"] = gr[\"product\"]/gr[\"count\"]\n",
        "    gr = gr[[\"clusters\", \"movieId\", \"contribute_score\"]]\n",
        "    return gr\n",
        "\n",
        "def get_shortlist(self, ratings, interaction_list= None, limit = 1000, cluster_num = 5):\n",
        "    if interaction_list is None:\n",
        "        interaction_list_, _ = get_interaction_set(  ratings, max_item = max_item, top_k_item = top_k_item  )\n",
        "    else:\n",
        "        interaction_list_ = interaction_list.copy()\n",
        "\n",
        "    # Predict score for each user\n",
        "    # scores = self.encoder(self._preprocess( [interaction_list_[itemCol], interaction_list_[\"y\"]], padding_size = wu_size )).numpy()\n",
        "    scores = self.minibatch_clustering(interaction_list_, batch_size=512)\n",
        "\n",
        "    # Limit number of cluster for each user\n",
        "    interaction_list_ = get_top_cluster(scores, interaction_list_)\n",
        "\n",
        "    # Get shortlist for each cluster\n",
        "    ratings_ = ratings.copy().set_index(\"userId\")\n",
        "    ratings_ = ratings_[ratings_[\"y\"]>0]\n",
        "    ratings_ = ratings_.join(interaction_list_, rsuffix=\"_l\", how = \"inner\")\n",
        "\n",
        "    # ratings_ = ratings_.explode([\"clusters\", \"scores\"])\n",
        "    # ratings_[\"contribute_score\"] = ratings_[\"scores\"].astype(\"float64\")*ratings_[\"y\"]\n",
        "\n",
        "    # ratings_ = ratings_.groupby([\"clusters\", \"movieId\"]).agg({\n",
        "    #     \"contribute_score\":\"mean\"#, \"contribute_score\":\"count\"\n",
        "    # }).reset_index()\n",
        "    ratings_ = chunk_explode(ratings_)\n",
        "\n",
        "    ratings_[\"rank\"] = ratings_.groupby(\"clusters\")[\"contribute_score\"].rank(method='first', ascending=False)\n",
        "    # ratings_[ratings_[\"clusters\"]==0].sort_values(by=\"rank\")\n",
        "\n",
        "    ratings_ = ratings_[(ratings_[\"rank\"] <= limit)&(ratings_[\"contribute_score\"]>0)]\n",
        "\n",
        "    self.shortlist = ratings_\n",
        "\n",
        "\n",
        "def get_recommendation(self, warm_up= None, historical_ratings = None, top_k = 10, is_remove_interacted = True):\n",
        "    \"\"\"shortlist: df: dataframe with cluster and list item by cluster\n",
        "    warm_up_item: \"\"\"\n",
        "    if warm_up is None:\n",
        "        warm_up, _ = get_interaction_set( \n",
        "                     historical_ratings\n",
        "                    , max_item = max_item\n",
        "                    , top_k_item = top_k_item )\n",
        "\n",
        "\n",
        "    scores = self.encoder(self._preprocess( [ warm_up[itemCol], warm_up[\"y\"]], padding_size = wu_size )).numpy()\n",
        "    wu = get_top_cluster( scores, warm_up)\n",
        "\n",
        "    wu = wu.explode([\"clusters\", \"scores\"]).reset_index()[[userCol, \"clusters\", \"scores\"]]\n",
        "    \n",
        "    wu = wu.merge(self.shortlist, on=\"clusters\", how='inner')\n",
        "    wu[\"matched_score\"] = wu[\"scores\"]*wu[\"contribute_score\"]\n",
        "    wu = wu.groupby([userCol, itemCol]).agg({\"matched_score\":'mean'}).reset_index()\n",
        "\n",
        "    if historical_ratings is None:\n",
        "        blacklist = warm_up.explode(itemCol).reset_index()\n",
        "        blacklist = blacklist[userCol].astype('str')+\"&\"+blacklist[itemCol].astype('str')\n",
        "    else:\n",
        "        blacklist = historical_ratings[userCol].astype('str')+\"&\"+historical_ratings[itemCol].astype('str')\n",
        "\n",
        "    if is_remove_interacted:\n",
        "        # wu = wu.merge(warm_up, on=userCol, how = 'inner', suffixes = ('', '_old'))\n",
        "        # wu[\"is_remove\"] = wu.apply(lambda x: x[itemCol] in x[itemCol+'_old'], axis=1)\n",
        "        # wu = wu[wu[\"is_remove\"]==False]\n",
        "        wu_key = wu[userCol].astype('str')+\"&\"+wu[itemCol].astype('str')\n",
        "        key_diff = set(wu_key).difference(blacklist)\n",
        "        where_diff = wu_key.isin(key_diff)\n",
        "        wu = wu[where_diff]\n",
        "\n",
        "    wu[\"rank\"] = wu.groupby(userCol)[\"matched_score\"].rank(method='first', ascending=False)\n",
        "\n",
        "    wu = wu[wu[\"rank\"]<= top_k]\n",
        "    wu = wu[[userCol, itemCol, \"rank\"]]\n",
        "    return wu\n",
        "\n",
        "Efficient_Rec.get_top_cluster = get_top_cluster\n",
        "Efficient_Rec.minibatch_clustering = minibatch_clustering\n",
        "Efficient_Rec.get_shortlist = get_shortlist\n",
        "Efficient_Rec.get_recommendation= get_recommendation"
      ],
      "metadata": {
        "id": "OU0eejkQB3lI"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xây dựng encoder model\n",
        "Encoder =  interaction embedding + user feature embedding<br> \n",
        "interaction embedding = sum( interaction embedding các item i)<br> \n",
        "interaction embedding item i = rating x (embedding id sản phẩm + embedding item feature)<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "Sbp1tH7yUeI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Vectorize (encode + padding) item list\n",
        "max_vocab_size = len(top_items) # nếu số item có <= top_k_item => lấy số lượng item max\n",
        "items_str = ' '.join([str(i) for i in top_items])\n",
        "itemStr = itemCol+\"_str\"\n",
        "\n",
        "vectorizer = layers.TextVectorization( max_tokens= top_k_item, split='whitespace', output_sequence_length= wu_size, name = 'vectorizer')\n",
        "vectorizer.adapt( [items_str] ) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxNyuo9-WJJG",
        "outputId": "8a5a84c4-9219-4585-f55f-c53099e6afef"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 149 ms, sys: 6.01 ms, total: 155 ms\n",
            "Wall time: 153 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Broadcasting_Multiply(tf.keras.layers.Layer):\n",
        "    \"\"\"Nhân 2 layers khác shape với nhau, trong đó:\n",
        "    inputs=[layer1, layer2]\n",
        "    layer1.shape = (None, n_item, n_feature)\n",
        "    layer2.shape = (None, n_item)\n",
        "    (Chú ý đúng thứ tự)\n",
        "    \"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, y = inputs\n",
        "        deno = tf.expand_dims(tf.cast(tf.math.count_nonzero(y, axis=1), tf.float32), -1)\n",
        "        #we add the extra dimension:\n",
        "        y = K.expand_dims(y, axis=-1)\n",
        "        #we replicate the elements\n",
        "        y = K.repeat_elements(y, rep=x.shape[2], axis=-1)\n",
        "\n",
        "        return x * y, deno"
      ],
      "metadata": {
        "id": "1U7EQG6zWpi9"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Xây dựng mạng\n",
        "embedding_size = 173\n",
        "reps_size = 132\n",
        "cluster_num = 43\n",
        "\n",
        "@tf.function\n",
        "def avg_layer(z):\n",
        "    t = K.sum(z[0], axis=1)/z[1]\n",
        "    t = tf.clip_by_value( t, -1, 1 )\n",
        "    t = tf.where(tf.math.is_nan(t), tf.zeros_like(t), t)\n",
        "    return t\n",
        "\n",
        "\n",
        "def interaction_embedding2():\n",
        "\n",
        "    input_wi = layers.Input(shape=(1,), name='input_wi')\n",
        "    wi = vectorizer(input_wi)\n",
        "    wi = layers.Embedding(input_dim= max_vocab_size, output_dim= embedding_size, mask_zero= True, name='ei')(wi)\n",
        "    # wi = layers.Dense(embedding_size, activation='sigmoid', use_bias = False, name='di')(wi)\n",
        "    wi = layers.Dense(embedding_size, activation='relu', use_bias = False, name='di1')(wi)\n",
        "    wi = layers.Dense(embedding_size, activation='relu', use_bias = False, name='di2')(wi)\n",
        "    # wi = layers.Dense(embedding_size, activation='sigmoid', use_bias = False, name='di3')(wi)\n",
        "\n",
        "    wr = layers.Input(shape=(wu_size,), name='warm_up_ratings')\n",
        "\n",
        "    ireps = Broadcasting_Multiply(name='mul')([wi, wr])\n",
        "    uprofile = layers.Lambda(lambda z: avg_layer(z) )(ireps)\n",
        "\n",
        "    uprofile = layers.Dense( reps_size, activation='relu', name='du1')(uprofile)\n",
        "    uprofile = layers.Dense( reps_size, activation='relu', name='du2')(uprofile)\n",
        "    # uprofile = layers.Dense( reps_size, activation='relu', name='du3')(uprofile)\n",
        "    # uprofile = layers.BatchNormalization(name='norm')(uprofile)\n",
        "    uprofile = layers.LayerNormalization(name='norm')(uprofile)\n",
        "    # uprofile = layers.Dense( reps_size, activation='relu', name='du4')(uprofile)\n",
        "    uprofile = layers.Dense(cluster_num, activation='sigmoid', name='clustering')(uprofile)\n",
        "    \n",
        "    \n",
        "    model = tf.keras.Model(inputs= [input_wi, wr], outputs=[uprofile])\n",
        "    return model"
      ],
      "metadata": {
        "id": "--fbIsGLXjPD"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of layer interaction embedding step by step\n",
        "# input_wi = [\"15 25 65 20 84\",  # 5 items\n",
        "#             \"51 54 45 21 24 83 81 76 74 75 72 48 29 38\",# 14 items\n",
        "#             \" \",] \n",
        "\n",
        "# tvectorizer = layers.TextVectorization( max_tokens= 17, split='whitespace', output_sequence_length= 10)\n",
        "# tvectorizer.adapt( input_wi ) \n",
        "\n",
        "# wi = tvectorizer(input_wi)\n",
        "# print(wi)\n",
        "# wi = layers.Embedding(input_dim= 17, output_dim= 4, mask_zero= True, name='ei')(wi)\n",
        "# print(wi)\n",
        "# wi = layers.Dense(3, activation='sigmoid',  use_bias = False, name='di')(wi)\n",
        "\n",
        "# wr = np.array([[0.5, 0.1, -0.5, 1, 0.25, 0, 0, 0, 0, 0], [0.25, 0.15, 0.5, 1, 0.25, 0.5, 0.1, -0.9, 0.4, -0.3], [0,0,0,0,0,0,0,0,0,0]])\n",
        "\n",
        "# ireps = Broadcasting_Multiply(name='mul')([wi, wr])\n",
        "# print(ireps)\n",
        "# uprofile = layers.Lambda(lambda z: avg_layer(z) )(ireps)\n",
        "# print(uprofile)\n",
        "\n",
        "# uprofile = layers.Dense( reps_size, activation='relu', name='du2')(uprofile)\n",
        "# uprofile = layers.LayerNormalization(name='norm')(uprofile)\n",
        "# uprofile = layers.Dense(5, activation='sigmoid', name='clustering')(uprofile)\n",
        "# print(uprofile)"
      ],
      "metadata": {
        "id": "a7EW-lg9Cfi5"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kiểm tra tham số\n",
        "# interaction_embedding2().summary()"
      ],
      "metadata": {
        "id": "cJMxOoxzXvr_"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.utils.plot_model( interaction_embedding2() ,show_shapes=True, show_dtype=True, show_layer_names=True )"
      ],
      "metadata": {
        "id": "PuAM8w01Xy93"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate model results"
      ],
      "metadata": {
        "id": "BISfMUNM37ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluate(model, movies, df):\n",
        "    dfu, ttop_items = get_interaction_set( df\n",
        "                    , max_item = max_item\n",
        "                    , top_k_item = top_k_item )\n",
        "    group_scores = model.encoder(model._preprocess( [dfu[itemCol], dfu[\"y\"]], padding_size = wu_size )).numpy()\n",
        "\n",
        "    print(\"SAMPLE INTERACTION EMBEDDING\")\n",
        "    print( np.max(group_scores), np.mean(group_scores), np.min(group_scores) )\n",
        "    print( group_scores[0:3] )\n",
        "\n",
        "    print(\"FEATURE PLOT\")\n",
        "    feature_plot( movies, df, group_scores)\n",
        "    \n",
        "    print(\"CLUSTER CHECKING\")\n",
        "    check_cluster(group_scores)\n",
        "    \n",
        "    print(\"SPECTROGRAM PLOT\")\n",
        "    plot_spectrogram(group_scores)\n",
        "\n",
        "def get_label(movies, df, is_encode = False):\n",
        "    movies[\"genres_list\"] = movies[\"genres\"].apply(lambda x: x.split(' '))\n",
        "    movie_genres = movies.explode(\"genres_list\")\n",
        "    gr = df.merge(movie_genres, on=\"movieId\").groupby([\"userId\", \"genres_list\"])[\"movieId\"].count().reset_index()\n",
        "    gr[\"rank\"] = gr.groupby(\"userId\")[\"movieId\"].rank(method='first', ascending=False)\n",
        "\n",
        "    labels = gr[gr[\"rank\"] ==1].set_index(\"userId\")\n",
        "    # labels[\"pred_max_ind\"] = np.argmax(group_scores, axis=1)\n",
        "\n",
        "    if is_encode:\n",
        "        label_enc = LabelEncoder()\n",
        "        labels[\"label\"] = label_enc.fit_transform(labels[\"genres_list\"])\n",
        "\n",
        "    return labels\n",
        "\n",
        "def feature_plot( movies, df, group_scores ):\n",
        "    tlabels = get_label(movies, df)\n",
        "\n",
        "    tsne = PCA(n_components=2, random_state=123)\n",
        "    # tsne = TSNE(n_components=2, random_state=123)\n",
        "    z = tsne.fit_transform(group_scores) \n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    df[\"y\"] = tlabels[\"genres_list\"]\n",
        "    df[\"comp-1\"] = z[:,0]\n",
        "    df[\"comp-2\"] = z[:,1]\n",
        "\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
        "                    palette=\"Paired\" ,#sns.color_palette(\"hls\", 3),\n",
        "                    data=df)#.set(title=\"Iris data T-SNE projection\") \n",
        "    plt.show()\n",
        "\n",
        "def check_cluster(group_scores):\n",
        "    # Kiểm tra số user trong mỗi cụm có bị vón cục\n",
        "    ugs= np.argmax(group_scores, axis=1)\n",
        "    for i in range(50):\n",
        "        print(i,': ', np.sum(ugs==i) )\n",
        "\n",
        "def plot_spectrogram(group_scores):\n",
        "    # Sort theo user_group + draw sigmoid/softmax layer\n",
        "    plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "    k =100\n",
        "    a = group_scores\n",
        "    ind = np.argmax(group_scores, axis=1)\n",
        "    plt.imshow( a[np.argsort(ind)][0:k] )\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "83VIKSPGNrN6"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_plot(model, movies, df):\n",
        "    dfu, ttop_items = get_interaction_set( df\n",
        "                    , max_item = max_item\n",
        "                    , top_k_item = top_k_item )\n",
        "    group_scores = model.encoder(model._preprocess( [dfu[itemCol], dfu[\"y\"]], padding_size = wu_size )).numpy()\n",
        "    print(\"FEATURE PLOT\")\n",
        "    feature_plot( movies, df, group_scores)\n",
        "    "
      ],
      "metadata": {
        "id": "_cs4Ci70vrkA"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Warm start user"
      ],
      "metadata": {
        "id": "xfUOMI8subTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "u_train_from = 0\n",
        "u_train_to = u_train_from+100000\n",
        "u_test = u_train_to+ 5000\n",
        "\n",
        "def get_labeled_data(df):\n",
        "    interact_df, _ = get_interaction_set( \n",
        "                     df\n",
        "                    , max_item = max_item\n",
        "                    , top_k_item = top_k_item )\n",
        "    labels = get_label( movies, df, is_encode = True)   \n",
        "    interact_df[\"label\"] = labels[\"label\"]\n",
        "    return interact_df[[\"movieId\",\"y\",\"label\"]]\n",
        "try:\n",
        "    train\n",
        "except:\n",
        "    # if exists, do not rerun\n",
        "    train =  get_labeled_data( warm_up_mask[(warm_up_mask[userCol]>u_train_from)&(warm_up_mask[userCol]<u_train_to)] )\n",
        "    train[\"rating_num\"] = train.apply(lambda x: len(x[\"y\"]), axis=1)\n",
        "    # pretrain with warm start user\n",
        "    train_warm = train[train[\"rating_num\"]>=100]\n",
        "\n",
        "    # train =  get_labeled_data( warm_up_mask[(warm_up_mask[userCol]>u_train_from)&(warm_up_mask[userCol]<u_train_to)] )\n",
        "train_warm.count()"
      ],
      "metadata": {
        "id": "iQtBh9xFunkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa855a3-8143-4c58-f97e-1a4b747791a2"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 10.2 ms, sys: 2.98 ms, total: 13.2 ms\n",
            "Wall time: 13.8 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE0hPo7UFNkV",
        "outputId": "2fc74af3-836f-43ee-96d5-22c39d5fd279"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1756"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constrastive model"
      ],
      "metadata": {
        "id": "pQokgKdL33ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cossim(a, b):\n",
        "    def l2(a):\n",
        "        return tf.sqrt( tf.reduce_sum(tf.square(a), axis=1) )\n",
        "    return tf.reduce_sum(tf.multiply(a, b), axis=1)/(l2(a)*l2(b))\n",
        "    \n",
        "def constrastive_loss(margin=0.01):\n",
        "    def compute_loss(wu_vec, mask_vec, negative_vec):\n",
        "        ap_distance = tf.linalg.norm(wu_vec - mask_vec, axis=1)\n",
        "        an_distance = tf.linalg.norm(wu_vec - negative_vec, axis=1)\n",
        "        # ap_distance = 1-cossim(wu_vec , mask_vec)\n",
        "        # an_distance = 1-cossim(wu_vec , negative_vec)\n",
        "        # Computing the Triplet Loss by subtracting both distances and\n",
        "        # making sure we don't get a negative value.\n",
        "        # ap_distance = tf.reduce_sum(tf.square(wu_vec - mask_vec), 1)\n",
        "        # an_distance = tf.reduce_sum(tf.square(wu_vec - negative_vec), 1)\n",
        "        loss = ap_distance - an_distance\n",
        "        # print(loss)\n",
        "        loss = tf.maximum(loss + margin, 0.)\n",
        "        # tf.print(loss)\n",
        "        loss = tf.reduce_mean(loss)\n",
        "        return loss\n",
        "    return compute_loss\n",
        "\n",
        "def circle_loss(margin=0.01, gama=0.05):\n",
        "    def compute_loss(wu_vec, mask_vec, negative_vec):\n",
        "        sp = cossim(wu_vec , mask_vec)\n",
        "        sn = cossim(wu_vec , negative_vec)\n",
        "\n",
        "        loss = gama*tf.math.exp(sn - sp + margin, 0)\n",
        "        # loss = tf.math.log(1+ loss)\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "        return loss\n",
        "    return compute_loss\n",
        "\n",
        "def nce_loss(t=0.5):\n",
        "    def compute_loss(wu_vec, mask_vec, negative_vec):\n",
        "        sp = cossim(wu_vec , mask_vec)/t\n",
        "        sn = cossim(wu_vec , negative_vec)/t\n",
        "        loss = - tf.math.log( sp/(sp+sn) )\n",
        "        loss = tf.reduce_sum(loss)\n",
        "        return loss\n",
        "    return compute_loss\n",
        "\n",
        "\n",
        "# Compile model\n",
        "# model = Efficient_Rec( encoder = interaction_embedding2(), \n",
        "#                       reps = None, #get_reps_model(),\n",
        "#                       decoder = None ,# mask_label(),\n",
        "#                       use_tf_function=False)\n",
        "\n",
        "# # Configure the loss and optimizer\n",
        "# model.compile(\n",
        "#     optimizer=tf.optimizers.Adam(),\n",
        "#     loss= nce_loss(0.5)#constrastive_loss(margin= 1),\n",
        "# )"
      ],
      "metadata": {
        "id": "D5W_j9I7z5Qn"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# epochs= 5\n",
        "# test_user =warm_up_mask[warm_up_mask[userCol]<5000]#.sample(1000)\n",
        "# model_plot(model, movies, \n",
        "#                test_user)\n",
        "# for n in range(epochs):\n",
        "#   print(n, \"/\", epochs, \": \", model._constrastive_train_minibatch_step( train_warm.sample(n=5000), batch_size=512))\n",
        "#   model_plot(model, movies, \n",
        "#             test_user )\n",
        "#   gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOqPS8ISvA6d",
        "outputId": "e964024c-90b0-48db-bbf6-1340e50177df"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 8.34 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# epochs= 20\n",
        "# for n in range(epochs):\n",
        "#   print(n, \"/\", epochs, \": \", model._constrastive_train_minibatch_step( interac_df, batch_size = 520 ))\n",
        "# gc.collect()"
      ],
      "metadata": {
        "id": "spjiPreaqSF9"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.layers[0].layers"
      ],
      "metadata": {
        "id": "-Zd0z6nPKrbQ"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.expand_dims(np.roll(interac_df[\"movieId\"].apply(lambda x:' '.join([str(i) for i in x])), 1), -1)"
      ],
      "metadata": {
        "id": "_sq1qj0D4TY2"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# supervised constrastive "
      ],
      "metadata": {
        "id": "9wO4EX1_Z-h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SupervisedContrastiveLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, temperature=1, name=None):\n",
        "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
        "        # Normalize feature vectors\n",
        "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
        "        # Compute logits\n",
        "        logits = tf.divide(\n",
        "            tf.matmul(\n",
        "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
        "            ),\n",
        "            self.temperature,\n",
        "        )\n",
        "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)"
      ],
      "metadata": {
        "id": "hQ-pwUZafD__"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện training\n",
        "# TODO: sửa lại pd shift thành np.roll\n",
        "def _supervised_constrastive_train_step(self, inputs):\n",
        "    items_pd, ratings_pd, labels = inputs[\"movieId\"], inputs[\"y\"], inputs[\"label\"]\n",
        "    items, ratings = self._preprocess((items_pd, ratings_pd), wu_size)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Interaction embedding\n",
        "        vec = self.encoder([items, ratings])\n",
        "\n",
        "        average_loss = self.loss(labels, vec)\n",
        "\n",
        "    # Apply an optimization step\n",
        "    variables = self.trainable_variables \n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    # gradients = [None if gradient is None else tf.clip_by_value(gradient, -0.1, 0.1)\n",
        "    #              for gradient in gradients]\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "\n",
        "Efficient_Rec._supervised_constrastive_train_step = _supervised_constrastive_train_step"
      ],
      "metadata": {
        "id": "TbltflgfUmXf"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện minibatch training\n",
        "# def _spv_constrastive_train_minibatch_step(self, inputs, batch_size):\n",
        "#     # df1, df2, df3 = inputs[0].copy(), inputs[1].copy(), inputs[2].copy()\n",
        "#     get_copy = lambda inputs: (inputs[0].copy(), inputs[1].copy(), inputs[2].copy())\n",
        "#     get_chunk_df = lambda df:[df[i:i+batch_size] for i in range(0,df.shape[0],batch_size)]\n",
        "#     get_chunk = lambda inputs: [get_chunk_df(df) for df in inputs]\n",
        "#     copies = get_copy( inputs )\n",
        "#     chunks = get_chunk(copies)\n",
        "\n",
        "#     get_chunk_i =  lambda chunks, i: (chunk[i] for chunk in chunks)\n",
        "#     losses = []\n",
        "#     for i in range(math.ceil(inputs[0].shape[0]/batch_size)):\n",
        "#         chunk = get_chunk_i(chunks, i)\n",
        "#         loss = self._supervised_constrastive_train_step(chunk)\n",
        "#         losses.append(loss[\"batch_loss\"].numpy())\n",
        "#         print(loss)\n",
        "#     return np.mean(losses)\n",
        "\n",
        "# Efficient_Rec._spv_constrastive_train_minibatch_step = _spv_constrastive_train_minibatch_step"
      ],
      "metadata": {
        "id": "aPG0aQ3vO4_n"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện minibatch training\n",
        "def _spv_constrastive_train_minibatch_step(self, inputs, batch_size):\n",
        "    df = inputs.copy()\n",
        "    chunks = [df[i:i+batch_size] for i in range(0,df.shape[0],batch_size)]\n",
        "    losses = []\n",
        "    for chunk in chunks:\n",
        "        loss = self._supervised_constrastive_train_step(chunk)\n",
        "        losses.append(loss[\"batch_loss\"].numpy())\n",
        "        print(loss)\n",
        "        gc.collect()\n",
        "    return np.mean(losses)\n",
        "\n",
        "Efficient_Rec._spv_constrastive_train_minibatch_step = _spv_constrastive_train_minibatch_step"
      ],
      "metadata": {
        "id": "qSr05iptaPha"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model = Efficient_Rec( encoder = interaction_embedding2(), \n",
        "                      reps = None, #get_reps_model(),\n",
        "                      decoder = None ,# mask_label(),\n",
        "                      use_tf_function=False)\n",
        "model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate = 0.001),\n",
        "    loss= TripletSemiHardLoss()#SupervisedContrastiveLoss(temperature = 0.05),\n",
        ")"
      ],
      "metadata": {
        "id": "uBCoJlJMdB7y"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train[\"movieId\"].apply(lambda x: int(len(x)/10)*10).to_frame(0).reset_index().groupby(0).count(\n",
        "# ).sort_values(\"userId\", ascending=False)"
      ],
      "metadata": {
        "id": "CJrj-BPMQ2lL"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start_u = 5000\n",
        "# test_set = warm_up_mask[(warm_up_mask[userCol]>start_u)&(warm_up_mask[userCol]<(start_u+5000))]\n",
        "# model_plot(model, movies, test_set )"
      ],
      "metadata": {
        "id": "V4jF7Vw7-UQU"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# epochs= 5\n",
        "# test_set = warm_up_mask[(warm_up_mask[userCol]>10000)&(warm_up_mask[userCol]<15000)]\n",
        "# model_plot(model, movies, test_set )\n",
        "# for n in range(epochs):\n",
        "#   print(n, \"/\", epochs, \": \", model._spv_constrastive_train_minibatch_step(train_warm.sample(frac=0.1), batch_size=512))\n",
        "#   model_plot(model, movies, test_set )\n",
        "#   gc.collect()"
      ],
      "metadata": {
        "id": "VoepslyXhaQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7846855-0e98-4615-edbf-f57c2f401bf1"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 6.44 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_evaluate(model, movies, \n",
        "#                warm_up_mask[warm_up_mask[userCol]<5000])"
      ],
      "metadata": {
        "id": "BKXC3Y6tRgt2"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_evaluate(model, movies, \n",
        "#                warm_up_mask[(warm_up_mask[userCol]>u_train_to)&(warm_up_mask[userCol]<(u_train_to+5000))])"
      ],
      "metadata": {
        "id": "ZZUCY20TSjFk"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Triplet loss\n",
        "Triplet loss với warm up, mask lấy từ interaction cùng 1 người, negative chọn từ một người khác không cùng category"
      ],
      "metadata": {
        "id": "8UgNHOHFtLg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: chuyển note sau thành file .py"
      ],
      "metadata": {
        "id": "Nj6qqxfAtrtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WU-Mask triplet loss\n",
        "import tensorflow as tf\n",
        "from tensorflow_addons.losses import metric_learning\n",
        "from tensorflow_addons.utils.keras_utils import LossFunctionWrapper\n",
        "from tensorflow_addons.utils.types import FloatTensorLike, TensorLike\n",
        "from typeguard import typechecked\n",
        "from typing import Optional, Union, Callable\n",
        "\n",
        "\n",
        "def _masked_minimum(data, mask, dim=1):\n",
        "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
        "    Args:\n",
        "      data: 2-D float `Tensor` of size [n, m].\n",
        "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
        "      dim: The dimension over which to compute the minimum.\n",
        "    Returns:\n",
        "      masked_minimums: N-D `Tensor`.\n",
        "        The minimized dimension is of size 1 after the operation.\n",
        "    \"\"\"\n",
        "    axis_maximums = tf.math.reduce_max(data, dim, keepdims=True)\n",
        "    masked_minimums = (\n",
        "        tf.math.reduce_min(\n",
        "            tf.math.multiply(data - axis_maximums, mask), dim, keepdims=True\n",
        "        )\n",
        "        + axis_maximums\n",
        "    )\n",
        "    return masked_minimums\n",
        "\n",
        "\n",
        "# @tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
        "@tf.function\n",
        "def triplet_hard_warmup_mask_loss(\n",
        "    y_true: TensorLike,\n",
        "    y_pred: TensorLike,\n",
        "    margin: FloatTensorLike = 1.0,\n",
        "    soft: bool = False,\n",
        "    distance_metric: Union[str, Callable] = \"L2\",\n",
        ") -> tf.Tensor:\n",
        "    \"\"\"Computes the triplet loss with hard negative and hard positive mining.\n",
        "    Args:\n",
        "      y_true: 1-D integer `Tensor` with shape [batch_size] of\n",
        "        multiclass integer labels.\n",
        "      y_pred: 2-D float `Tensor` of embedding vectors. Embeddings should\n",
        "        be l2 normalized.\n",
        "      margin: Float, margin term in the loss definition.\n",
        "      soft: Boolean, if set, use the soft margin version.\n",
        "      distance_metric: str or function, determines distance metric:\n",
        "                       \"L2\" for l2-norm distance\n",
        "                       \"squared-L2\" for squared l2-norm distance\n",
        "                       \"angular\" for cosine similarity\n",
        "                        A custom function returning a 2d adjacency\n",
        "                          matrix of a chosen distance metric can\n",
        "                          also be passed here. e.g.\n",
        "                          def custom_distance(batch):\n",
        "                              batch = 1 - batch @ batch.T\n",
        "                              return batch\n",
        "                          triplet_semihard_loss(batch, labels,\n",
        "                                        distance_metric=custom_distance\n",
        "                                    )\n",
        "    Returns:\n",
        "      triplet_loss: float scalar with dtype of y_pred.\n",
        "    \"\"\"\n",
        "\n",
        "    labels, embeddings = y_true, y_pred\n",
        "    wu_embeddings, mask_embeddings = embeddings\n",
        "\n",
        "    def convert_to_float(embeddings):\n",
        "        convert_to_float32 = (\n",
        "            embeddings.dtype == tf.dtypes.float16 or embeddings.dtype == tf.dtypes.bfloat16\n",
        "        )\n",
        "        precise_wu_embeddings = (\n",
        "            tf.cast(embeddings, tf.dtypes.float32) if convert_to_float32 else embeddings\n",
        "        )\n",
        "        return precise_wu_embeddings, convert_to_float32\n",
        "\n",
        "    precise_wu_embeddings, convert_to_float32_wu = convert_to_float(wu_embeddings)\n",
        "    precise_mask_embeddings, convert_to_float32_mask = convert_to_float(mask_embeddings)\n",
        "\n",
        "    convert_to_float32 = (convert_to_float32_wu or convert_to_float32_mask)\n",
        "\n",
        "    # Reshape label tensor to [batch_size, 1].\n",
        "    lshape = tf.shape(labels)\n",
        "    labels = tf.reshape(labels, [lshape[0], 1])\n",
        "\n",
        "    # Build pairwise squared distance matrix.\n",
        "    if distance_metric == \"L2\":\n",
        "        pdist_matrix = metric_learning.pairwise_distance(\n",
        "            precise_wu_embeddings, squared=False\n",
        "        )\n",
        "        hard_positives =  metric_learning.pairwise_distance(\n",
        "            precise_wu_embeddings - precise_mask_embeddings, squared=False\n",
        "        )\n",
        "\n",
        "    elif distance_metric == \"squared-L2\":\n",
        "        pdist_matrix = metric_learning.pairwise_distance(\n",
        "            precise_wu_embeddings, squared=True\n",
        "        )\n",
        "        hard_positives =  metric_learning.pairwise_distance(\n",
        "            precise_wu_embeddings - precise_mask_embeddings, squared=True\n",
        "        )\n",
        "\n",
        "    elif distance_metric == \"angular\":\n",
        "        pdist_matrix = metric_learning.angular_distance(precise_wu_embeddings)\n",
        "        hard_positives =  metric_learning.angular_distance(\n",
        "            precise_wu_embeddings - precise_mask_embeddings\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        pdist_matrix = distance_metric(precise_wu_embeddings)\n",
        "        hard_positives =  distance_metric( precise_wu_embeddings - precise_mask_embeddings )\n",
        "    \n",
        "\n",
        "    # Build pairwise binary adjacency matrix.\n",
        "    adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
        "    # Invert so we can select negatives only.\n",
        "    adjacency_not = tf.math.logical_not(adjacency)\n",
        "\n",
        "    adjacency_not = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n",
        "    # hard negatives: smallest D_an.\n",
        "    hard_negatives = _masked_minimum(pdist_matrix, adjacency_not)\n",
        "\n",
        "    hard_positives = tf.expand_dims(hard_positives, -1)\n",
        "    \n",
        "\n",
        "    if soft:\n",
        "        triplet_loss = tf.math.log1p(tf.math.exp(hard_positives - hard_negatives))\n",
        "    else:\n",
        "        triplet_loss = tf.maximum(hard_positives - hard_negatives + margin, 0.0)\n",
        "\n",
        "    # Get final mean triplet loss\n",
        "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
        "\n",
        "    if convert_to_float32:\n",
        "        return tf.cast(triplet_loss, embeddings.dtype)\n",
        "    else:\n",
        "        return triplet_loss\n",
        "\n",
        "\n",
        "\n",
        "# @tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
        "class TripletHardWarmupMaskedLoss(LossFunctionWrapper):\n",
        "    \"\"\"Computes the triplet loss with hard negative and hard positive mining.\n",
        "    The loss encourages the maximum positive distance (between a pair of embeddings\n",
        "    with the same labels) to be smaller than the minimum negative distance plus the\n",
        "    margin constant in the mini-batch.\n",
        "    The loss selects the hardest positive and the hardest negative samples\n",
        "    within the batch when forming the triplets for computing the loss.\n",
        "    See: https://arxiv.org/pdf/1703.07737.\n",
        "    We expect labels `y_true` to be provided as 1-D integer `Tensor` with shape\n",
        "    [batch_size] of multi-class integer labels. And embeddings `y_pred` must be\n",
        "    2-D float `Tensor` of l2 normalized embedding vectors.\n",
        "    Args:\n",
        "      margin: Float, margin term in the loss definition. Default value is 1.0.\n",
        "      soft: Boolean, if set, use the soft margin version. Default value is False.\n",
        "      name: Optional name for the op.\n",
        "    \"\"\"\n",
        "\n",
        "    @typechecked\n",
        "    def __init__(\n",
        "        self,\n",
        "        margin: FloatTensorLike = 1.0,\n",
        "        soft: bool = False,\n",
        "        distance_metric: Union[str, Callable] = \"L2\",\n",
        "        name: Optional[str] = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            triplet_hard_warmup_mask_loss,\n",
        "            name=name,\n",
        "            reduction=tf.keras.losses.Reduction.NONE,\n",
        "            margin=margin,\n",
        "            soft=soft,\n",
        "            distance_metric=distance_metric,\n",
        "        )"
      ],
      "metadata": {
        "id": "t8vcPXVTtoUi"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Triplet ver1 với sửa lại với negative sample: so sánh khoảng cách của toàn bộ tương tác của người dùng ancol với negative (thay vì warm-up ancol với warm-up negative), với positive giữ nguyên (so sánh warm up và mask)"
      ],
      "metadata": {
        "id": "vkNRLv6h-Xg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WU-Mask triplet loss\n",
        "import tensorflow as tf\n",
        "from tensorflow_addons.losses import metric_learning\n",
        "from tensorflow_addons.utils.keras_utils import LossFunctionWrapper\n",
        "from tensorflow_addons.utils.types import FloatTensorLike, TensorLike\n",
        "from typeguard import typechecked\n",
        "from typing import Optional, Union, Callable\n",
        "\n",
        "\n",
        "def _masked_minimum(data, mask, dim=1):\n",
        "    \"\"\"Computes the axis wise minimum over chosen elements.\n",
        "    Args:\n",
        "      data: 2-D float `Tensor` of size [n, m].\n",
        "      mask: 2-D Boolean `Tensor` of size [n, m].\n",
        "      dim: The dimension over which to compute the minimum.\n",
        "    Returns:\n",
        "      masked_minimums: N-D `Tensor`.\n",
        "        The minimized dimension is of size 1 after the operation.\n",
        "    \"\"\"\n",
        "    axis_maximums = tf.math.reduce_max(data, dim, keepdims=True)\n",
        "    masked_minimums = (\n",
        "        tf.math.reduce_min(\n",
        "            tf.math.multiply(data - axis_maximums, mask), dim, keepdims=True\n",
        "        )\n",
        "        + axis_maximums\n",
        "    )\n",
        "    return masked_minimums\n",
        "\n",
        "\n",
        "# @tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
        "@tf.function\n",
        "def triplet_hard_warmup_mask_loss2(\n",
        "    y_true: TensorLike,\n",
        "    y_pred: TensorLike,\n",
        "    margin: FloatTensorLike = 1.0,\n",
        "    soft: bool = False,\n",
        "    distance_metric: Union[str, Callable] = \"L2\",\n",
        ") -> tf.Tensor:\n",
        "    \"\"\"Computes the triplet loss with hard negative and hard positive mining.\n",
        "    Args:\n",
        "      y_true: 1-D integer `Tensor` with shape [batch_size] of\n",
        "        multiclass integer labels.\n",
        "      y_pred: 2-D float `Tensor` of embedding vectors. Embeddings should\n",
        "        be l2 normalized.\n",
        "      margin: Float, margin term in the loss definition.\n",
        "      soft: Boolean, if set, use the soft margin version.\n",
        "      distance_metric: str or function, determines distance metric:\n",
        "                       \"L2\" for l2-norm distance\n",
        "                       \"squared-L2\" for squared l2-norm distance\n",
        "                       \"angular\" for cosine similarity\n",
        "                        A custom function returning a 2d adjacency\n",
        "                          matrix of a chosen distance metric can\n",
        "                          also be passed here. e.g.\n",
        "                          def custom_distance(batch):\n",
        "                              batch = 1 - batch @ batch.T\n",
        "                              return batch\n",
        "                          triplet_semihard_loss(batch, labels,\n",
        "                                        distance_metric=custom_distance\n",
        "                                    )\n",
        "    Returns:\n",
        "      triplet_loss: float scalar with dtype of y_pred.\n",
        "    \"\"\"\n",
        "\n",
        "    labels, embeddings = y_true, y_pred\n",
        "    wu_embeddings, mask_embeddings = embeddings\n",
        "\n",
        "    def convert_to_float(embeddings):\n",
        "        convert_to_float32 = (\n",
        "            embeddings.dtype == tf.dtypes.float16 or embeddings.dtype == tf.dtypes.bfloat16\n",
        "        )\n",
        "        precise_wu_embeddings = (\n",
        "            tf.cast(embeddings, tf.dtypes.float32) if convert_to_float32 else embeddings\n",
        "        )\n",
        "        return precise_wu_embeddings, convert_to_float32\n",
        "\n",
        "    precise_wu_embeddings, convert_to_float32_wu = convert_to_float(wu_embeddings)\n",
        "    precise_mask_embeddings, convert_to_float32_mask = convert_to_float(mask_embeddings)\n",
        "    precise_embeddings = tf.math.add(precise_wu_embeddings, precise_mask_embeddings)\n",
        "\n",
        "    convert_to_float32 = (convert_to_float32_wu or convert_to_float32_mask)\n",
        "\n",
        "    # Reshape label tensor to [batch_size, 1].\n",
        "    lshape = tf.shape(labels)\n",
        "    labels = tf.reshape(labels, [lshape[0], 1])\n",
        "\n",
        "    # Build pairwise squared distance matrix.\n",
        "    if distance_metric == \"L2\":\n",
        "        pdist_matrix = metric_learning.pairwise_distance(\n",
        "            precise_embeddings, squared=False\n",
        "        )\n",
        "        hard_positives =  metric_learning.pairwise_distance(\n",
        "            precise_wu_embeddings - precise_mask_embeddings, squared=False\n",
        "        )\n",
        "\n",
        "    elif distance_metric == \"squared-L2\":\n",
        "        pdist_matrix = metric_learning.pairwise_distance(\n",
        "            precise_embeddings, squared=True\n",
        "        )\n",
        "        hard_positives =  metric_learning.pairwise_distance(\n",
        "            precise_wu_embeddings - precise_mask_embeddings, squared=True\n",
        "        )\n",
        "\n",
        "    elif distance_metric == \"angular\":\n",
        "        pdist_matrix = metric_learning.angular_distance(precise_embeddings)\n",
        "        hard_positives =  metric_learning.angular_distance(\n",
        "            precise_wu_embeddings - precise_mask_embeddings\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        pdist_matrix = distance_metric(precise_embeddings)\n",
        "        hard_positives =  distance_metric( precise_wu_embeddings - precise_mask_embeddings )\n",
        "    \n",
        "\n",
        "    # Build pairwise binary adjacency matrix.\n",
        "    adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
        "    # Invert so we can select negatives only.\n",
        "    adjacency_not = tf.math.logical_not(adjacency)\n",
        "\n",
        "    adjacency_not = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n",
        "    # hard negatives: smallest D_an.\n",
        "    hard_negatives = _masked_minimum(pdist_matrix, adjacency_not)\n",
        "\n",
        "    hard_positives = tf.expand_dims(hard_positives, -1)\n",
        "    \n",
        "\n",
        "    if soft:\n",
        "        triplet_loss = tf.math.log1p(tf.math.exp(hard_positives - hard_negatives))\n",
        "    else:\n",
        "        triplet_loss = tf.maximum(hard_positives - hard_negatives + margin, 0.0)\n",
        "\n",
        "    # Get final mean triplet loss\n",
        "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
        "\n",
        "    if convert_to_float32:\n",
        "        return tf.cast(triplet_loss, embeddings.dtype)\n",
        "    else:\n",
        "        return triplet_loss\n",
        "\n",
        "\n",
        "\n",
        "# @tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
        "class TripletHardWarmupMaskedLoss2(LossFunctionWrapper):\n",
        "    \"\"\"Computes the triplet loss with hard negative and hard positive mining.\n",
        "    The loss encourages the maximum positive distance (between a pair of embeddings\n",
        "    with the same labels) to be smaller than the minimum negative distance plus the\n",
        "    margin constant in the mini-batch.\n",
        "    The loss selects the hardest positive and the hardest negative samples\n",
        "    within the batch when forming the triplets for computing the loss.\n",
        "    See: https://arxiv.org/pdf/1703.07737.\n",
        "    We expect labels `y_true` to be provided as 1-D integer `Tensor` with shape\n",
        "    [batch_size] of multi-class integer labels. And embeddings `y_pred` must be\n",
        "    2-D float `Tensor` of l2 normalized embedding vectors.\n",
        "    Args:\n",
        "      margin: Float, margin term in the loss definition. Default value is 1.0.\n",
        "      soft: Boolean, if set, use the soft margin version. Default value is False.\n",
        "      name: Optional name for the op.\n",
        "    \"\"\"\n",
        "\n",
        "    @typechecked\n",
        "    def __init__(\n",
        "        self,\n",
        "        margin: FloatTensorLike = 1.0,\n",
        "        soft: bool = False,\n",
        "        distance_metric: Union[str, Callable] = \"L2\",\n",
        "        name: Optional[str] = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            triplet_hard_warmup_mask_loss2,\n",
        "            name=name,\n",
        "            reduction=tf.keras.losses.Reduction.NONE,\n",
        "            margin=margin,\n",
        "            soft=soft,\n",
        "            distance_metric=distance_metric,\n",
        "        )"
      ],
      "metadata": {
        "id": "sc5GuC6--WBr"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện training\n",
        "def _wu_mask_triplet_train_step(self, inputs):\n",
        "    items_pd, ratings_pd, labels = inputs[\"movieId\"], inputs[\"y\"], inputs[\"label\"]\n",
        "    warm_up, mask = self._wu_mask_split2(inputs.sample(frac=1), mask_ratio = 0.5)\n",
        "    wu_items, wu_ratings = self._preprocess(warm_up, wu_size)\n",
        "    mask_item, mask_ratings = self._preprocess(mask, wu_size)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Interaction embedding\n",
        "        wu_vec = self.encoder([wu_items, wu_ratings])\n",
        "        mask_vec = self.encoder([mask_item, mask_ratings])\n",
        "\n",
        "        average_loss = self.loss( labels,  (wu_vec, mask_vec)) \n",
        "\n",
        "    # Apply an optimization step\n",
        "    variables = self.trainable_variables \n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    # gradients = [None if gradient is None else tf.clip_by_value(gradient, -0.1, 0.1)\n",
        "    #              for gradient in gradients]\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "\n",
        "Efficient_Rec._wu_mask_triplet_train_step = _wu_mask_triplet_train_step"
      ],
      "metadata": {
        "id": "jJ2lvQx-ukRu"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện minibatch training\n",
        "def _wu_mask_triplet_train_minibatch_step(self, inputs, batch_size):\n",
        "    df = inputs.copy()\n",
        "    chunks = [df[i:i+batch_size] for i in range(0,df.shape[0],batch_size)]\n",
        "    losses = []\n",
        "    for chunk in chunks:\n",
        "        loss = self._wu_mask_triplet_train_step(chunk)\n",
        "        losses.append(loss[\"batch_loss\"].numpy())\n",
        "        print(loss)\n",
        "        gc.collect()\n",
        "    return np.mean(losses)\n",
        "\n",
        "Efficient_Rec._wu_mask_triplet_train_minibatch_step = _wu_mask_triplet_train_minibatch_step"
      ],
      "metadata": {
        "id": "ubXjhvrgxfbo"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model = Efficient_Rec( encoder = interaction_embedding2(), \n",
        "                      use_tf_function=False)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=TripletHardWarmupMaskedLoss2(soft=True))"
      ],
      "metadata": {
        "id": "eOjkNyaZtgBM"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "epochs= 1\n",
        "test_set = warm_up_mask[(warm_up_mask[userCol]>1000)&(warm_up_mask[userCol]<2000)]\n",
        "model_plot( model, movies, test_set )\n",
        "for n in range(epochs):\n",
        "  print(n+1, \"/\", epochs, \": \", model._wu_mask_triplet_train_minibatch_step(train_warm.sample(frac=0.1), batch_size=512))\n",
        "  model_plot(model, movies, test_set )\n",
        "  gc.collect()"
      ],
      "metadata": {
        "id": "nPrQu5yGtpDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac778aa9-cf47-40fb-befe-4a0c5104398d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
            "Wall time: 5.72 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "WSAWNkXjGX7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5417245f-7209-4b75-8ac5-e3a6330b01f8"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_evaluate(model, movies, \n",
        "#                warm_up_mask[(warm_up_mask[userCol]>u_train_to)&(warm_up_mask[userCol]<(u_train_to+5000))])"
      ],
      "metadata": {
        "id": "WGxSqovoy8UD"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pick item pipeline"
      ],
      "metadata": {
        "id": "CgfpZRoGF1KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "ratings1 = warm_up_mask\n",
        "limit = 500\n",
        "cluster_num = 3\n",
        "\n",
        "interaction_list_, _ = get_interaction_set( ratings1, max_item = max_item, top_k_item = top_k_item )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qu6bPpWCGGka",
        "outputId": "753764d1-81a9-4828-ed76-3b67fbaa62f9"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 54.5 s, sys: 1.55 s, total: 56 s\n",
            "Wall time: 56 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Predict score for each user\n",
        "scores = model.minibatch_clustering(interaction_list_, batch_size=512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7N-WEgnL9oX",
        "outputId": "7ced27a4-c842-4f97-ddc7-d930ac41b7b3"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3min 23s, sys: 1.97 s, total: 3min 25s\n",
            "Wall time: 1min 52s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Limit number of cluster for each user\n",
        "interaction_list_ = get_top_cluster(scores, interaction_list_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywqwa3t5HE55",
        "outputId": "61814c59-55f9-418c-b6f4-3e79c4655a92"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.1 s, sys: 15 ms, total: 2.11 s\n",
            "Wall time: 2.12 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interaction_list_.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVTMYEBJV3Bc",
        "outputId": "b51aebf8-0949-4dfe-9a54-cc6148dca368"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(138493, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Get shortlist for each cluster\n",
        "ratings_ = ratings1.copy().set_index(\"userId\")\n",
        "ratings_ = ratings_[ratings_[\"y\"]>0]\n",
        "ratings_ = ratings_.join(interaction_list_, rsuffix=\"_l\", how = \"inner\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjDkjhakGWXz",
        "outputId": "65070a07-34ad-485d-ee3e-3195c9bb6d2d"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.69 s, sys: 363 ms, total: 6.05 s\n",
            "Wall time: 6.01 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG0-VqKdfudN",
        "outputId": "7a50788d-67e8-487e-b5d4-764f375575d7"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12365724, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "def chunk_explode(ratings, batch_size = 1024**2):\n",
        "    chunks = [ratings[i:i+batch_size] for i in range(0,ratings.shape[0],batch_size)]\n",
        "    explodes = []\n",
        "\n",
        "    # Todo: convert for loop to parallel\n",
        "    for chunk in chunks:\n",
        "        explode = chunk.explode([\"clusters\", \"scores\"])\n",
        "        explode[\"contribute_score\"] = explode[\"scores\"].astype(\"float64\")*explode[\"y\"]\n",
        "        # return explode\n",
        "        explode = explode.groupby([\"clusters\", \"movieId\"]).agg({\n",
        "            \"contribute_score\": [\"mean\", \"count\"]\n",
        "        }).reset_index()\n",
        "        explode.columns = [\"clusters\", \"movieId\", \"mean\", \"count\"]\n",
        "        explodes.append(explode)\n",
        "\n",
        "    # combine results\n",
        "    gr = pd.concat(explodes, axis = 0)\n",
        "    gr[\"product\"] = gr[\"mean\"]*gr[\"count\"]\n",
        "    gr = gr.groupby([\"clusters\", \"movieId\"]).sum().reset_index()\n",
        "    gr[\"contribute_score\"] = gr[\"product\"]/gr[\"count\"]\n",
        "    gr = gr[[\"clusters\", \"movieId\", \"contribute_score\"]]\n",
        "    return gr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKzrCTwNZQuc",
        "outputId": "45bc4aa3-ef03-4635-e145-760b16a2f84c"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
            "Wall time: 9.78 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# ratings_ = ratings_.explode([\"clusters\", \"scores\"])\n",
        "# ratings_[\"contribute_score\"] = ratings_[\"scores\"].astype(\"float64\")*ratings_[\"y\"]\n",
        "\n",
        "# ratings_ = ratings_.groupby([\"clusters\", \"movieId\"]).agg({\n",
        "#     \"contribute_score\":\"mean\"#, \"contribute_score\":\"count\"\n",
        "# }).reset_index()\n",
        "ratings_ = chunk_explode(ratings_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeqqlnK3XpkQ",
        "outputId": "8856f37d-c4a6-4ef4-d0cb-604f69c46809"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 36 s, sys: 443 ms, total: 36.5 s\n",
            "Wall time: 36.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_[\"rank\"] = ratings_.groupby(\"clusters\")[\"contribute_score\"].rank(method='first', ascending=False)\n",
        "# ratings_[ratings_[\"clusters\"]==0].sort_values(by=\"rank\")\n",
        "\n",
        "ratings_ = ratings_[(ratings_[\"rank\"] <= limit)&(ratings_[\"contribute_score\"]>0)]\n",
        "\n",
        "model.shortlist = ratings_"
      ],
      "metadata": {
        "id": "EIiDkBdfGYei"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "# START\n",
        "model.get_shortlist( warm_up_mask, limit = 500, cluster_num = 5)"
      ],
      "metadata": {
        "id": "fQtrddWcEaoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51ecf33a-55be-42d6-8094-05597fd93b93"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5min 3s, sys: 4.56 s, total: 5min 8s\n",
            "Wall time: 3min 36s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EvrD1tOVkXD",
        "outputId": "41b0c886-b4ad-45fc-98f0-add7bad2e913"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "# interaction_list = train_warm[train_warm.index<=3000]\n",
        "top_k = 50\n",
        "is_remove_interacted = True\n",
        "\n",
        "y_pred = model.get_recommendation( historical_ratings= warm_up_mask, \n",
        "                                  top_k = top_k, is_remove_interacted = True)"
      ],
      "metadata": {
        "id": "a1MUU9bdHQAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "xbwdvfsrVhXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_true = ratings[(ratings[\"y\"]>0)&(ratings[userCol]<=3000)][[userCol, itemCol, \"y\"]]\n",
        "y_true = test"
      ],
      "metadata": {
        "id": "2dGbwAJOur5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# eval_map = map_at_k(y_true, y_pred, col_prediction='rank', col_rating=\"y\", k=top_k)\n",
        "# eval_ndcg = ndcg_at_k(y_true, y_pred, col_prediction='rank', col_rating=\"y\", k=top_k)\n",
        "eval_precision = precision_at_k(y_true, y_pred, col_user = userCol, col_item = itemCol ,col_prediction='rank', col_rating=\"y\", k=top_k)\n",
        "eval_recall = recall_at_k(y_true, y_pred, col_user = userCol, col_item = itemCol, col_prediction='rank', col_rating=\"y\", k=top_k)\n",
        "\n",
        "print('K = %f' % top_k)\n",
        "print(\n",
        "    # \"MAP:\\t%f\" % eval_map,\n",
        "    #   \"NDCG:\\t%f\" % eval_ndcg,\n",
        "      \"Precision@K:\\t%f\" % eval_precision,\n",
        "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
      ],
      "metadata": {
        "id": "yZKN6iNvJQFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rs(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    y_true: dataframe: user_id, item_id, y (rating normailised), only favorite item\n",
        "    y_pred: dataframe: user_id, item_id (just top k item)\n",
        "    return:\n",
        "    precision@k, recall@k\n",
        "    \"\"\"\n",
        "    total1 = y_true.merge(y_pred, on=[userCol, itemCol], how = 'outer', suffixes=('_t', '_p'))\n",
        "    total1['is_pt'] = total1.apply(lambda x: 0 if (np.isnan(x[\"y\"]) or np.isnan(x[\"rank\"]) ) else 1,axis=1)\n",
        "    total = total1.groupby(userCol).agg({\n",
        "        \"y\":'count',\n",
        "        \"rank\":'count',\n",
        "        \"is_pt\": 'sum'\n",
        "        })\n",
        "    total.columns = [\"true_num\", \"predict_num\", \"pred_true_num\"]\n",
        "    total[\"macro_p\"] = total[\"pred_true_num\"]/total[\"predict_num\"]\n",
        "    total[\"macro_r\"] = total[\"pred_true_num\"]/total[\"true_num\"]\n",
        "\n",
        "    total = total[total[\"predict_num\"]>0]\n",
        "\n",
        "    macro_p = total[total[\"predict_num\"]>0][\"macro_p\"].mean()\n",
        "    macro_r = total[total[\"true_num\"]>0][\"macro_r\"].mean()\n",
        "\n",
        "    micro_p = total[\"pred_true_num\"].sum()/total[\"predict_num\"].sum()\n",
        "    micro_r = total[\"pred_true_num\"].sum()/total[\"true_num\"].sum()\n",
        "\n",
        "    print(\"macro_p: \", macro_p, \"; macro_r :\", macro_r)\n",
        "    print(\"micro_p: \", micro_p, \"; micro_r :\", micro_r)\n",
        "\n",
        "    return macro_p, macro_r, micro_p, micro_r"
      ],
      "metadata": {
        "id": "QPPnAXS9vBhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "macro_p, macro_r, micro_p, micro_r= evaluate_rs(y_true, y_pred)"
      ],
      "metadata": {
        "id": "HM00AWaQ1H1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1/0"
      ],
      "metadata": {
        "id": "Mnm-7SBjOU5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total1 = y_true.merge(y_pred, on=[userCol, itemCol], how = 'outer', suffixes=('_t', '_p'))\n",
        "total1['is_pt'] = total1.apply(lambda x: 0 if (np.isnan(x[\"y\"]) or np.isnan(x[\"rank\"]) ) else 1,axis=1)\n",
        "total = total1.groupby(userCol).agg({\n",
        "    \"y\":'count',\n",
        "    \"rank\":'count',\n",
        "    \"is_pt\": 'sum'\n",
        "    })\n",
        "total.columns = [\"true_num\", \"predict_num\", \"pred_true_num\"]\n",
        "total[\"macro_p\"] = total[\"pred_true_num\"]/total[\"predict_num\"]\n",
        "total[\"macro_r\"] = total[\"pred_true_num\"]/total[\"true_num\"]\n",
        "\n",
        "total = total[total[\"predict_num\"]>0]\n",
        "\n",
        "macro_p = total[total[\"predict_num\"]>0][\"macro_p\"].mean()\n",
        "macro_r = total[(total[\"true_num\"]>0) & (total[\"predict_num\"]>0)][\"macro_r\"].mean()\n",
        "\n",
        "micro_p = total[\"pred_true_num\"].sum()/total[\"predict_num\"].sum()\n",
        "micro_r = total[\"pred_true_num\"].sum()/total[\"true_num\"].sum()\n",
        "\n",
        "print(\"macro_p: \", macro_p, \"; macro_r :\", macro_r)\n",
        "print(\"micro_p: \", micro_p, \"; micro_r :\", micro_r)"
      ],
      "metadata": {
        "id": "jZsyQTl82HYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total"
      ],
      "metadata": {
        "id": "Du28sLPn3SuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "lHXHyIUJy6Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TableA = pd.DataFrame(np.random.rand(4, 3),\n",
        "                      pd.Index(list('aacd'), name='Key'),\n",
        "                      ['A', 'B', 'C']).reset_index()\n",
        "TableB = pd.DataFrame(np.random.rand(4, 3),\n",
        "                      pd.Index(list('abff'), name='Key'),\n",
        "                      ['A', 'B', 'C']).reset_index()"
      ],
      "metadata": {
        "id": "nphWtj6wGNz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TableA"
      ],
      "metadata": {
        "id": "QfiX1_xlGjX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TableB"
      ],
      "metadata": {
        "id": "BuY8q0xtGkag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(TableB.Key).difference(TableA.Key)"
      ],
      "metadata": {
        "id": "aeIbMYPKKfHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify what values are in TableB and not in TableA\n",
        "key_diff = set(TableB.Key).difference(TableA.Key)\n",
        "where_diff = TableB.Key.isin(key_diff)\n",
        "\n",
        "# Slice TableB accordingly and append to TableA\n",
        "TableA.append(TableB[where_diff], ignore_index=True)"
      ],
      "metadata": {
        "id": "FYkJETJGGL4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TableB[where_diff]"
      ],
      "metadata": {
        "id": "GqyCkh_QGo50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "where_diff"
      ],
      "metadata": {
        "id": "6mkG0UZFKIv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = TableA.astype(\"str\")\n",
        "x[\"A\"]+x[\"B\"]"
      ],
      "metadata": {
        "id": "uy-Gs_ZUNEU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "4BbEPlHH31zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dxQAE_KuKWZH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "v2.2_ML20M_sequence.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "17fHxkHoAQq6"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}