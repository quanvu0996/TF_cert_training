{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quanvu0996/TF_cert_training/blob/main/v2.5_ML20M%20optimizing%20cluster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZQDJKGUAQqn",
        "outputId": "c87a9781-bc7c-4f6f-d1c7-e8f44ab5f7eb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67.5\n",
            "svmem(total=13617745920, available=12824690688, percent=5.8, used=532615168, free=11188420608, active=901509120, inactive=1335062528, buffers=108154880, cached=1788555264, shared=1224704, slab=127156224)\n",
            "5.8\n",
            "94.17631055345758\n"
          ]
        }
      ],
      "source": [
        "import psutil\n",
        "# gives a single float value\n",
        "print(psutil.cpu_percent())\n",
        "# gives an object with many fields\n",
        "print(psutil.virtual_memory())\n",
        "# you can convert that object to a dictionary print(dict(psutil.virtual_memory()._asdict()))\n",
        "# you can have the percentage of used RAM\n",
        "print(psutil.virtual_memory().percent)\n",
        "# you can calculate percentage of available memory\n",
        "print(psutil.virtual_memory().available * 100 / psutil.virtual_memory().total)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7sPzaTKcdTo",
        "outputId": "8a05b737-28f9-4fdc-a6b5-07c5a02dd4a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.17.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libs"
      ],
      "metadata": {
        "id": "oGKXb_a2GGDk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XS6UHXPVAQqs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "import gc\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.losses import TripletSemiHardLoss \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Gv8gXemwD3UQ",
        "outputId": "0c88b245-945a-442f-8002-8e0a12409a73"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Crhty6fiAQqu"
      },
      "outputs": [],
      "source": [
        "itemCol = 'movieId'\n",
        "userCol = 'userId'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DGX setup\n",
        "# fpath = \"./ml-20m\" \n",
        "\n",
        "#colab setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "fpath = \"/content/gdrive/MyDrive/RECOMMENDER_STUDIES/data/ml-20m\""
      ],
      "metadata": {
        "id": "xOJJy441A0w0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe3f9a04-2edb-4ea8-e259-67ee0e69587b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data\n",
        "ratings = pd.read_csv(fpath+'/ratings.csv')\n",
        "\n",
        "ratings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "jQck8YHxCPJ1",
        "outputId": "fe933ee8-3f53-4d3f-ca08-91a68b21612d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          userId  movieId  rating   timestamp\n",
              "0              1        2     3.5  1112486027\n",
              "1              1       29     3.5  1112484676\n",
              "2              1       32     3.5  1112484819\n",
              "3              1       47     3.5  1112484727\n",
              "4              1       50     3.5  1112484580\n",
              "...          ...      ...     ...         ...\n",
              "20000258  138493    68954     4.5  1258126920\n",
              "20000259  138493    69526     4.5  1259865108\n",
              "20000260  138493    69644     3.0  1260209457\n",
              "20000261  138493    70286     5.0  1258126944\n",
              "20000262  138493    71619     2.5  1255811136\n",
              "\n",
              "[20000263 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fb2d2479-313b-451b-80ad-bbdf06c9dd49\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112486027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>50</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1112484580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000258</th>\n",
              "      <td>138493</td>\n",
              "      <td>68954</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1258126920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000259</th>\n",
              "      <td>138493</td>\n",
              "      <td>69526</td>\n",
              "      <td>4.5</td>\n",
              "      <td>1259865108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000260</th>\n",
              "      <td>138493</td>\n",
              "      <td>69644</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1260209457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000261</th>\n",
              "      <td>138493</td>\n",
              "      <td>70286</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1258126944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20000262</th>\n",
              "      <td>138493</td>\n",
              "      <td>71619</td>\n",
              "      <td>2.5</td>\n",
              "      <td>1255811136</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000263 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fb2d2479-313b-451b-80ad-bbdf06c9dd49')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fb2d2479-313b-451b-80ad-bbdf06c9dd49 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fb2d2479-313b-451b-80ad-bbdf06c9dd49');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize rating về dạng -1 -> 1 (-1 = ghét, 1 = thích)\n",
        "ratings[\"y\"] = ratings[\"rating\"]/2.5-1\n",
        "\n",
        "# Kiểm tra rating trong khoảng -1 -> 1\n",
        "ratings.groupby(\"y\")[\"y\"].count().plot(kind='bar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "BrwYHetxCiIw",
        "outputId": "38bc2238-cf93-4faf-ba0e-dd6f6c650d64"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f81e7bcd810>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading movie\n",
        "movies = pd.read_csv(fpath+'/movies.csv')\n",
        "movies[\"year\"]=movies[\"title\"].apply(lambda x: x[-5:-1])\n",
        "movies[\"genres\"] = movies[\"genres\"].apply(lambda x: ' ' if x == '(no genres listed)' else ' '.join(x.split('|')) )\n",
        "movies[\"title\"]= movies[\"title\"].apply(lambda x: x[0:-7])\n",
        "movies.head()"
      ],
      "metadata": {
        "id": "_DH8SxPH64nZ",
        "outputId": "99b73574-a02d-4017-b166-3a75749dc620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   movieId                        title  \\\n",
              "0        1                    Toy Story   \n",
              "1        2                      Jumanji   \n",
              "2        3             Grumpier Old Men   \n",
              "3        4            Waiting to Exhale   \n",
              "4        5  Father of the Bride Part II   \n",
              "\n",
              "                                        genres  year  \n",
              "0  Adventure Animation Children Comedy Fantasy  1995  \n",
              "1                   Adventure Children Fantasy  1995  \n",
              "2                               Comedy Romance  1995  \n",
              "3                         Comedy Drama Romance  1995  \n",
              "4                                       Comedy  1995  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-41b2319b-5f41-4255-9fba-10ebc3c48181\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movieId</th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Toy Story</td>\n",
              "      <td>Adventure Animation Children Comedy Fantasy</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Jumanji</td>\n",
              "      <td>Adventure Children Fantasy</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Grumpier Old Men</td>\n",
              "      <td>Comedy Romance</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Waiting to Exhale</td>\n",
              "      <td>Comedy Drama Romance</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Father of the Bride Part II</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41b2319b-5f41-4255-9fba-10ebc3c48181')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-41b2319b-5f41-4255-9fba-10ebc3c48181 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-41b2319b-5f41-4255-9fba-10ebc3c48181');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model v2.3\n",
        "Sử dụng constrastive loss thay cho onehot"
      ],
      "metadata": {
        "id": "HfA78F8HRVJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chia dữ liệu thành các tập cho model clustering, nhãn recommendation và test\n",
        "#     warm_up_mask: ratings đã quan sát\n",
        "#     target: rating dùng để đánh giá kết quả recommend của module recommend cho từng người dùng\n",
        "#     test: đánh giá độc lập\n",
        "# TODO: chia tập dữ liệu theo user-wise => đánh giá với những user hoàn toàn mới thì model có học được không?\n",
        "# v2.1. gộp warm_up và mask thành 1\n",
        "def dataset_split(ratings):\n",
        "    train, test = train_test_split(ratings, test_size= 0.25)\n",
        "    warm_up_mask, target = train_test_split(train, test_size= 0.25)\n",
        "    return warm_up_mask, target, test\n",
        "\n",
        "warm_up_mask, target, test = dataset_split(ratings)"
      ],
      "metadata": {
        "id": "90sKne1ZRc5F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kiểm tra số lượng ratings\n",
        "warm_up_mask.shape[0], target.shape[0], test.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiRtTNy1ScIa",
        "outputId": "5cbbe3ae-57bb-42a7-dc86-c8d9eaf8ef61"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11250147, 3750050, 5000066)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# v2.2: chỉ groupby, không padding\n",
        "def get_interaction_set(interaction, max_item = None, top_k_item = None):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        interaction: df[userCol, itemCol, y]: dữ liệu đầu vào\n",
        "        max_item: int: item num limit\n",
        "    Output:\n",
        "        df, itemCol: list, y: list, itemCol_str: string, userCol as index\n",
        "        list item sắp xếp theo giảm dần độ lớn rating\n",
        "    \"\"\"\n",
        "    items = ratings.groupby(itemCol).count().sort_values(by=userCol, ascending=False)\n",
        "    if top_k_item is not None:\n",
        "        top_items = items.head(top_k_item).index\n",
        "        interaction = interaction[interaction[itemCol].isin(top_items)]\n",
        "    else:\n",
        "        top_items = items.index\n",
        "\n",
        "    # Sắp xếp item theo thứ tự giảm dần rating (về sau cắt padding sẽ ưu tiên giữ lại item có rating cao)\n",
        "    rindex = interaction.groupby(userCol)[\"y\"].transform(lambda grp: grp.sort_values(ascending=False).index)\n",
        "    interaction = interaction.reindex(rindex)\n",
        "    \n",
        "    # Chuyển thành warm-up set theo từng user\n",
        "    interaction = interaction.groupby(\"userId\").agg({itemCol:list, \"y\":list})\n",
        "\n",
        "    # Giới hạn độ dài warm_up size\n",
        "    if max_item is not None:\n",
        "        interaction[itemCol] = interaction[itemCol].apply(lambda x: x[0:max_item])\n",
        "        interaction[\"y\"] = interaction[\"y\"].apply(lambda x: x[0:max_item])\n",
        "\n",
        "    return interaction, top_items\n",
        "\n",
        "top_k_item = 20000\n",
        "wu_size = 200\n",
        "mask_size = 200\n",
        "max_item = wu_size + mask_size\n",
        "\n",
        "# interac_df, top_items = get_interaction_set(warm_up_mask[warm_up_mask[userCol]<3000]\n",
        "#                     , max_item = max_item\n",
        "#                     , top_k_item = top_k_item )\n",
        "# interac_df\n",
        "\n",
        "# define top_items\n",
        "top_items = ratings.groupby(itemCol).count().sort_values(by=userCol, ascending=False).head(top_k_item).index"
      ],
      "metadata": {
        "id": "C3ukIA2SS00l"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(top_items)"
      ],
      "metadata": {
        "id": "Oo-X6mlUn-CP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class  model"
      ],
      "metadata": {
        "id": "HIfY6FeqKqQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo cấu hình: xác định encoder, decoder, mapping dims\n",
        "class Efficient_Rec(tf.keras.Model):\n",
        "  def __init__(self, encoder, decoder= None, reps=None, use_tf_function=False):\n",
        "    super().__init__()\n",
        "    self.use_tf_function = use_tf_function\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.reps = reps\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)"
      ],
      "metadata": {
        "id": "XtrIkMaRbo6v"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _wu_mask_split(self, batch_inputs, mask_ratio = 0.25):\n",
        "    \"Chia ratings và items thành mask, warm up\"\n",
        "    input_items, input_ratings = batch_inputs[itemCol], batch_inputs[\"y\"]\n",
        "\n",
        "    def list_split(input, mask_ratio = 0.25, seed= 42):\n",
        "        return train_test_split(input[0:(wu_size+mask_size)], test_size= mask_ratio, random_state=seed)\n",
        "\n",
        "    seed = random.randint(1, 100)\n",
        "    items = input_items.apply(lambda x: list_split(x, mask_ratio, seed))\n",
        "    ratings = input_ratings.apply(lambda x: list_split(x, mask_ratio, seed))\n",
        "\n",
        "    wu_item_list = items.apply(lambda x: x[0])\n",
        "    mask_item_list = items.apply(lambda x: x[1])\n",
        "\n",
        "    wu_rating_list = ratings.apply(lambda x: x[0])\n",
        "    mask_rating_list = ratings.apply(lambda x: x[1])\n",
        "    return (wu_item_list, wu_rating_list), (mask_item_list, mask_rating_list)\n",
        "\n",
        "Efficient_Rec._wu_mask_split = _wu_mask_split"
      ],
      "metadata": {
        "id": "mZtGCHBnjcCg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _wu_mask_split2(self, batch_inputs, mask_ratio = 0.25):\n",
        "    \"Chia ratings và items thành mask, warm up, giữ nguyên full set trong warmup\"\n",
        "    input_items, input_ratings = batch_inputs[itemCol], batch_inputs[\"y\"]\n",
        "\n",
        "    def list_split(input, mask_ratio = 0.25, seed= 42):\n",
        "        return train_test_split(input[0:(wu_size+mask_size)], test_size= mask_ratio, random_state=seed)\n",
        "\n",
        "    seed = random.randint(1, 100)\n",
        "    items = input_items.apply(lambda x: list_split(x, mask_ratio, seed))\n",
        "    ratings = input_ratings.apply(lambda x: list_split(x, mask_ratio, seed))\n",
        "\n",
        "    wu_item_list = input_items #items.apply(lambda x: x[0])\n",
        "    mask_item_list = items.apply(lambda x: x[1])\n",
        "\n",
        "    wu_rating_list = input_ratings #ratings.apply(lambda x: x[0])\n",
        "    mask_rating_list = ratings.apply(lambda x: x[1])\n",
        "    return (wu_item_list, wu_rating_list), (mask_item_list, mask_rating_list)\n",
        "\n",
        "Efficient_Rec._wu_mask_split2 = _wu_mask_split2"
      ],
      "metadata": {
        "id": "38n-keEMtBQ7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _preprocess(self, inputs, padding_size = 100):\n",
        "    \"\"\"\n",
        "    Padding về wu_size và mask_size, convert list of items => string of items\n",
        "    batch_inputs: df: itemStr, y\"\"\"\n",
        "\n",
        "    def padding_list(list_item, wu_size, value=0, is_padding=True):\n",
        "        series_item1 = list_item[0:wu_size]\n",
        "        if is_padding:\n",
        "            series_item1 = series_item1+[value]*(wu_size-len(series_item1))\n",
        "        return series_item1\n",
        "\n",
        "    items_list, ratings_list = inputs\n",
        "\n",
        "    items   = items_list.apply(lambda x: ' '.join(list([str(i) for i in x])))\n",
        "    ratings =   np.stack( ratings_list.apply(lambda x: padding_list( x, padding_size  ) ) )\n",
        "\n",
        "    return items, ratings\n",
        "\n",
        "Efficient_Rec._preprocess = _preprocess"
      ],
      "metadata": {
        "id": "OvIJP8m7bvaF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện training\n",
        "def _train_step(self, inputs):\n",
        "    warm_up, mask = self._wu_mask_split(inputs, mask_ratio = 0.25)\n",
        "    wu_items, wu_ratings = self._preprocess(warm_up, wu_size)\n",
        "    mask_item, mask_ratings = self._preprocess(mask, mask_size)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Encode the input\n",
        "        wu_vec = self.encoder([wu_items, wu_ratings])\n",
        "        y_pred = self.reps(wu_vec)\n",
        "\n",
        "        # Encde the output\n",
        "        mask_vec = self.decoder([mask_item, mask_ratings])\n",
        "\n",
        "        average_loss = self.loss(mask_vec, y_pred)\n",
        "\n",
        "    # Apply an optimization step\n",
        "    variables = self.trainable_variables \n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "\n",
        "Efficient_Rec._train_step = _train_step"
      ],
      "metadata": {
        "id": "gzoNOhtuhHlu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện minibatch training\n",
        "def _batch_train_step(self, inputs, batch_size):\n",
        "    df = inputs.copy()\n",
        "    chunks = [df[i:i+batch_size] for i in range(0,df.shape[0],batch_size)]\n",
        "    for chunk in chunks:\n",
        "        self._train_step(chunk)\n",
        "\n",
        "Efficient_Rec._batch_train_step = _batch_train_step"
      ],
      "metadata": {
        "id": "HxyczAa_cLdY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện training\n",
        "def _constrastive_train_step(self, inputs):\n",
        "    warm_up, mask = self._wu_mask_split2(inputs.sample(frac=1), mask_ratio = 0.5)\n",
        "    wu_items, wu_ratings = self._preprocess(warm_up, wu_size)\n",
        "    mask_item, mask_ratings = self._preprocess(mask, wu_size)\n",
        "\n",
        "    # negative_items = wu_items.shift(1, axis=0, fill_value=['0 '])\n",
        "    negative_items = np.roll( wu_items.values, shift= 1, axis=0)\n",
        "    negative_items = pd.Series(negative_items)\n",
        "    negative_ratings =  np.roll( wu_ratings, shift= 1, axis=0)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Interaction embedding\n",
        "        wu_vec = self.encoder([wu_items, wu_ratings])\n",
        "        mask_vec = self.encoder([mask_item, mask_ratings])\n",
        "        negative_vec = self.encoder([negative_items, negative_ratings])\n",
        "\n",
        "        average_loss = self.loss(wu_vec, mask_vec, negative_vec) #- 0.1*np.mean([tf.linalg.norm(x) for x in self.trainable_variables])\n",
        "\n",
        "    # Apply an optimization step\n",
        "    variables = self.trainable_variables \n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    gradients = [None if gradient is None else tf.clip_by_value(gradient, -0.1, 0.1)\n",
        "                 for gradient in gradients]\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "\n",
        "Efficient_Rec._constrastive_train_step = _constrastive_train_step"
      ],
      "metadata": {
        "id": "vuBMn3DD0BaM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện minibatch training\n",
        "# @tf.function\n",
        "def _constrastive_train_minibatch_step(self, inputs, batch_size):\n",
        "    df = inputs.copy()\n",
        "    chunks = [df[i:i+batch_size] for i in range(0,df.shape[0],batch_size)]\n",
        "    losses = []\n",
        "    for chunk in chunks:\n",
        "        loss = self._constrastive_train_step(chunk)\n",
        "        losses.append(loss[\"batch_loss\"].numpy())\n",
        "        print(loss)\n",
        "    return np.mean(losses)\n",
        "\n",
        "Efficient_Rec._constrastive_train_minibatch_step = _constrastive_train_minibatch_step"
      ],
      "metadata": {
        "id": "8DG-5bLRqIxc"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# staticmethod\n",
        "def get_top_cluster(scores, interaction_list):\n",
        "    interaction_list_ = interaction_list.copy()\n",
        "    idx = np.argsort(-scores.transpose(),axis=0)[:cluster_num]\n",
        "    interaction_list_[\"clusters\"] = [list(i) for i in idx.transpose()]\n",
        "    interaction_list_[\"scores\"] = [ list(scores[i][ind]) for i, ind in enumerate(idx.transpose()) ]\n",
        "    return interaction_list_\n",
        "\n",
        "def get_shortlist(self, ratings, interaction_list= None, limit = 1000, cluster_num = 5):\n",
        "    if interaction_list is None:\n",
        "        interaction_list_, _ = get_interaction_set(  ratings, max_item = max_item, top_k_item = top_k_item  )\n",
        "    else:\n",
        "        interaction_list_ = interaction_list.copy()\n",
        "\n",
        "    # Predict score for each user\n",
        "    scores = self.encoder(self._preprocess( [interaction_list_[itemCol], interaction_list_[\"y\"]], padding_size = wu_size )).numpy()\n",
        "\n",
        "    # Limit number of cluster for each user\n",
        "    interaction_list_ = get_top_cluster(scores, interaction_list_)\n",
        "\n",
        "    # Get shortlist for each cluster\n",
        "    ratings_ = ratings.copy().set_index(\"userId\")\n",
        "    ratings_ = ratings_.join(interaction_list_, rsuffix=\"_l\", how = \"inner\")\n",
        "\n",
        "    ratings_ = ratings_.explode([\"clusters\", \"scores\"])\n",
        "    ratings_[\"contribute_score\"] = ratings_[\"scores\"].astype(\"float64\")*ratings_[\"y\"]\n",
        "\n",
        "    ratings_ = ratings_.groupby([\"clusters\", \"movieId\"]).agg({\n",
        "        \"contribute_score\":\"mean\"#, \"contribute_score\":\"count\"\n",
        "    }).reset_index()\n",
        "\n",
        "    ratings_[\"rank\"] = ratings_.groupby(\"clusters\")[\"contribute_score\"].rank(method='first', ascending=False)\n",
        "    # ratings_[ratings_[\"clusters\"]==0].sort_values(by=\"rank\")\n",
        "\n",
        "    ratings_ = ratings_[(ratings_[\"rank\"] <= limit)&(ratings_[\"contribute_score\"]>0)]\n",
        "\n",
        "    self.shortlist = ratings_\n",
        "\n",
        "\n",
        "def get_recommendation(self, warm_up, top_k = 10, is_remove_interacted = True):\n",
        "    \"\"\"shortlist: df: dataframe with cluster and list item by cluster\n",
        "    warm_up_item: \"\"\"\n",
        "    scores = self.encoder(self._preprocess( [ warm_up[itemCol], warm_up[\"y\"]], padding_size = wu_size )).numpy()\n",
        "    wu = get_top_cluster( scores, warm_up)\n",
        "\n",
        "    wu = wu.explode([\"clusters\", \"scores\"]).reset_index()[[userCol, \"clusters\", \"scores\"]]\n",
        "    \n",
        "    wu = wu.merge(self.shortlist, on=\"clusters\", how='inner')\n",
        "    wu[\"matched_score\"] = wu[\"scores\"]*wu[\"contribute_score\"]\n",
        "    wu = wu.groupby([userCol, itemCol]).agg({\"matched_score\":'mean'}).reset_index()\n",
        "\n",
        "    if is_remove_interacted:\n",
        "        wu = wu.merge(warm_up, on=userCol, how = 'inner', suffixes = ('', '_old'))\n",
        "        wu[\"is_remove\"] = wu.apply(lambda x: x[itemCol] in x[itemCol+'_old'], axis=1)\n",
        "        wu = wu[wu[\"is_remove\"]==False]\n",
        "\n",
        "    wu[\"rank\"] = wu.groupby(userCol)[\"matched_score\"].rank(method='first', ascending=False)\n",
        "\n",
        "    wu = wu[wu[\"rank\"]<= top_k]\n",
        "    wu = wu[[userCol, itemCol, \"rank\"]]\n",
        "    return wu\n",
        "\n",
        "Efficient_Rec.get_top_cluster = get_top_cluster\n",
        "Efficient_Rec.get_shortlist = get_shortlist\n",
        "Efficient_Rec.get_recommendation= get_recommendation"
      ],
      "metadata": {
        "id": "OU0eejkQB3lI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xây dựng encoder model\n",
        "Encoder =  interaction embedding + user feature embedding<br> \n",
        "interaction embedding = sum( interaction embedding các item i)<br> \n",
        "interaction embedding item i = rating x (embedding id sản phẩm + embedding item feature)<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "Sbp1tH7yUeI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Vectorize (encode + padding) item list\n",
        "max_vocab_size = len(top_items) # nếu số item có <= top_k_item => lấy số lượng item max\n",
        "items_str = ' '.join([str(i) for i in top_items])\n",
        "itemStr = itemCol+\"_str\"\n",
        "\n",
        "vectorizer = layers.TextVectorization( max_tokens= top_k_item, split='whitespace', output_sequence_length= wu_size, name = 'vectorizer')\n",
        "vectorizer.adapt( [items_str] ) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxNyuo9-WJJG",
        "outputId": "6c17a87f-dc55-418e-805c-a69dab24b823"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 212 ms, sys: 36.2 ms, total: 248 ms\n",
            "Wall time: 466 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Broadcasting_Multiply(tf.keras.layers.Layer):\n",
        "    \"\"\"Nhân 2 layers khác shape với nhau, trong đó:\n",
        "    inputs=[layer1, layer2]\n",
        "    layer1.shape = (None, n_item, n_feature)\n",
        "    layer2.shape = (None, n_item)\n",
        "    (Chú ý đúng thứ tự)\n",
        "    \"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, y = inputs\n",
        "        deno = tf.expand_dims(tf.cast(tf.math.count_nonzero(y, axis=1), tf.float32), -1)\n",
        "        #we add the extra dimension:\n",
        "        y = K.expand_dims(y, axis=-1)\n",
        "        #we replicate the elements\n",
        "        y = K.repeat_elements(y, rep=x.shape[2], axis=-1)\n",
        "\n",
        "        return x * y, deno"
      ],
      "metadata": {
        "id": "1U7EQG6zWpi9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Xây dựng mạng\n",
        "embedding_size = 173\n",
        "reps_size = 132\n",
        "cluster_num = 43\n",
        "\n",
        "@tf.function\n",
        "def avg_layer(z):\n",
        "    t = K.sum(z[0], axis=1)/z[1]\n",
        "    t = tf.clip_by_value( t, -1, 1 )\n",
        "    t = tf.where(tf.math.is_nan(t), tf.zeros_like(t), t)\n",
        "    return t\n",
        "\n",
        "\n",
        "def interaction_embedding2():\n",
        "\n",
        "    input_wi = layers.Input(shape=(1,), name='input_wi')\n",
        "    wi = vectorizer(input_wi)\n",
        "    wi = layers.Embedding(input_dim= max_vocab_size, output_dim= embedding_size, mask_zero= True, name='ei')(wi)\n",
        "    # wi = layers.Dense(embedding_size, activation='sigmoid', use_bias = False, name='di')(wi)\n",
        "    wi = layers.Dense(embedding_size, activation='relu', use_bias = False, name='di1')(wi)\n",
        "    wi = layers.Dense(embedding_size, activation='relu', use_bias = False, name='di2')(wi)\n",
        "    # wi = layers.Dense(embedding_size, activation='sigmoid', use_bias = False, name='di3')(wi)\n",
        "\n",
        "    wr = layers.Input(shape=(wu_size,), name='warm_up_ratings')\n",
        "\n",
        "    ireps = Broadcasting_Multiply(name='mul')([wi, wr])\n",
        "    uprofile = layers.Lambda(lambda z: avg_layer(z) )(ireps)\n",
        "\n",
        "    uprofile = layers.Dense( reps_size, activation='relu', name='du1')(uprofile)\n",
        "    uprofile = layers.Dense( reps_size, activation='relu', name='du2')(uprofile)\n",
        "    # uprofile = layers.Dense( reps_size, activation='relu', name='du3')(uprofile)\n",
        "    # uprofile = layers.BatchNormalization(name='norm')(uprofile)\n",
        "    uprofile = layers.LayerNormalization(name='norm')(uprofile)\n",
        "    # uprofile = layers.Dense( reps_size, activation='relu', name='du4')(uprofile)\n",
        "    uprofile = layers.Dense(cluster_num, activation='sigmoid', name='clustering')(uprofile)\n",
        "    \n",
        "    \n",
        "    model = tf.keras.Model(inputs= [input_wi, wr], outputs=[uprofile])\n",
        "    return model"
      ],
      "metadata": {
        "id": "--fbIsGLXjPD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of layer interaction embedding step by step\n",
        "# input_wi = [\"15 25 65 20 84\",  # 5 items\n",
        "#             \"51 54 45 21 24 83 81 76 74 75 72 48 29 38\",# 14 items\n",
        "#             \" \",] \n",
        "\n",
        "# tvectorizer = layers.TextVectorization( max_tokens= 17, split='whitespace', output_sequence_length= 10)\n",
        "# tvectorizer.adapt( input_wi ) \n",
        "\n",
        "# wi = tvectorizer(input_wi)\n",
        "# print(wi)\n",
        "# wi = layers.Embedding(input_dim= 17, output_dim= 4, mask_zero= True, name='ei')(wi)\n",
        "# print(wi)\n",
        "# wi = layers.Dense(3, activation='sigmoid',  use_bias = False, name='di')(wi)\n",
        "\n",
        "# wr = np.array([[0.5, 0.1, -0.5, 1, 0.25, 0, 0, 0, 0, 0], [0.25, 0.15, 0.5, 1, 0.25, 0.5, 0.1, -0.9, 0.4, -0.3], [0,0,0,0,0,0,0,0,0,0]])\n",
        "\n",
        "# ireps = Broadcasting_Multiply(name='mul')([wi, wr])\n",
        "# print(ireps)\n",
        "# uprofile = layers.Lambda(lambda z: avg_layer(z) )(ireps)\n",
        "# print(uprofile)\n",
        "\n",
        "# uprofile = layers.Dense( reps_size, activation='relu', name='du2')(uprofile)\n",
        "# uprofile = layers.LayerNormalization(name='norm')(uprofile)\n",
        "# uprofile = layers.Dense(5, activation='sigmoid', name='clustering')(uprofile)\n",
        "# print(uprofile)"
      ],
      "metadata": {
        "id": "a7EW-lg9Cfi5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kiểm tra tham số\n",
        "# interaction_embedding2().summary()"
      ],
      "metadata": {
        "id": "cJMxOoxzXvr_"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.utils.plot_model( interaction_embedding2() ,show_shapes=True, show_dtype=True, show_layer_names=True )"
      ],
      "metadata": {
        "id": "PuAM8w01Xy93"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate model results"
      ],
      "metadata": {
        "id": "BISfMUNM37ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluate(model, movies, df):\n",
        "    dfu, ttop_items = get_interaction_set( df\n",
        "                    , max_item = max_item\n",
        "                    , top_k_item = top_k_item )\n",
        "    group_scores = model.encoder(model._preprocess( [dfu[itemCol], dfu[\"y\"]], padding_size = wu_size )).numpy()\n",
        "\n",
        "    print(\"SAMPLE INTERACTION EMBEDDING\")\n",
        "    print( np.max(group_scores), np.mean(group_scores), np.min(group_scores) )\n",
        "    print( group_scores[0:3] )\n",
        "\n",
        "    print(\"FEATURE PLOT\")\n",
        "    feature_plot( movies, df, group_scores)\n",
        "    \n",
        "    print(\"CLUSTER CHECKING\")\n",
        "    check_cluster(group_scores)\n",
        "    \n",
        "    print(\"SPECTROGRAM PLOT\")\n",
        "    plot_spectrogram(group_scores)\n",
        "\n",
        "def get_label(movies, df, is_encode = False):\n",
        "    movies[\"genres_list\"] = movies[\"genres\"].apply(lambda x: x.split(' '))\n",
        "    movie_genres = movies.explode(\"genres_list\")\n",
        "    gr = df.merge(movie_genres, on=\"movieId\").groupby([\"userId\", \"genres_list\"])[\"movieId\"].count().reset_index()\n",
        "    gr[\"rank\"] = gr.groupby(\"userId\")[\"movieId\"].rank(method='first', ascending=False)\n",
        "\n",
        "    labels = gr[gr[\"rank\"] ==1].set_index(\"userId\")\n",
        "    # labels[\"pred_max_ind\"] = np.argmax(group_scores, axis=1)\n",
        "\n",
        "    if is_encode:\n",
        "        label_enc = LabelEncoder()\n",
        "        labels[\"label\"] = label_enc.fit_transform(labels[\"genres_list\"])\n",
        "\n",
        "    return labels\n",
        "\n",
        "def feature_plot( movies, df, group_scores ):\n",
        "    tlabels = get_label(movies, df)\n",
        "\n",
        "    tsne = PCA(n_components=2, random_state=123)\n",
        "    # tsne = TSNE(n_components=2, random_state=123)\n",
        "    z = tsne.fit_transform(group_scores) \n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    df[\"y\"] = tlabels[\"genres_list\"]\n",
        "    df[\"comp-1\"] = z[:,0]\n",
        "    df[\"comp-2\"] = z[:,1]\n",
        "\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
        "                    palette=\"Paired\" ,#sns.color_palette(\"hls\", 3),\n",
        "                    data=df)#.set(title=\"Iris data T-SNE projection\") \n",
        "    plt.show()\n",
        "\n",
        "def check_cluster(group_scores):\n",
        "    # Kiểm tra số user trong mỗi cụm có bị vón cục\n",
        "    ugs= np.argmax(group_scores, axis=1)\n",
        "    for i in range(50):\n",
        "        print(i,': ', np.sum(ugs==i) )\n",
        "\n",
        "def plot_spectrogram(group_scores):\n",
        "    # Sort theo user_group + draw sigmoid/softmax layer\n",
        "    plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "    k =100\n",
        "    a = group_scores\n",
        "    ind = np.argmax(group_scores, axis=1)\n",
        "    plt.imshow( a[np.argsort(ind)][0:k] )\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "83VIKSPGNrN6"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_plot(model, movies, df):\n",
        "    dfu, ttop_items = get_interaction_set( df\n",
        "                    , max_item = max_item\n",
        "                    , top_k_item = top_k_item )\n",
        "    group_scores = model.encoder(model._preprocess( [dfu[itemCol], dfu[\"y\"]], padding_size = wu_size )).numpy()\n",
        "    print(\"FEATURE PLOT\")\n",
        "    feature_plot( movies, df, group_scores)\n",
        "    "
      ],
      "metadata": {
        "id": "_cs4Ci70vrkA"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Warm start user"
      ],
      "metadata": {
        "id": "xfUOMI8subTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u_train_from = 0\n",
        "u_train_to = u_train_from+100000\n",
        "u_test = u_train_to+ 5000\n",
        "\n",
        "def get_labeled_data(df):\n",
        "    interact_df, _ = get_interaction_set( \n",
        "                     df\n",
        "                    , max_item = max_item\n",
        "                    , top_k_item = top_k_item )\n",
        "    labels = get_label( movies, df, is_encode = True)   \n",
        "    interact_df[\"label\"] = labels[\"label\"]\n",
        "    return interact_df[[\"movieId\",\"y\",\"label\"]]\n",
        "\n",
        "# if exists, do not rerun\n",
        "train =  get_labeled_data( warm_up_mask[(warm_up_mask[userCol]>u_train_from)&(warm_up_mask[userCol]<u_train_to)] )\n",
        "train[\"rating_num\"] = train.apply(lambda x: len(x[\"y\"]), axis=1)\n",
        "# pretrain with warm start user\n",
        "train_warm = train[train[\"rating_num\"]>=100]\n",
        "\n",
        "# train =  get_labeled_data( warm_up_mask[(warm_up_mask[userCol]>u_train_from)&(warm_up_mask[userCol]<u_train_to)] )\n",
        "train_warm.count()"
      ],
      "metadata": {
        "id": "iQtBh9xFunkU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6f8b62e-77e8-44bc-bff9-41e1c8e74d01"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "movieId       22000\n",
              "y             22000\n",
              "label         22000\n",
              "rating_num    22000\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE0hPo7UFNkV",
        "outputId": "67033406-0e6a-43f0-c6ef-d3a1c2947b11"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constrastive model"
      ],
      "metadata": {
        "id": "pQokgKdL33ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cossim(a, b):\n",
        "    def l2(a):\n",
        "        return tf.sqrt( tf.reduce_sum(tf.square(a), axis=1) )\n",
        "    return tf.reduce_sum(tf.multiply(a, b), axis=1)/(l2(a)*l2(b))\n",
        "    \n",
        "def constrastive_loss(margin=0.01):\n",
        "    def compute_loss(wu_vec, mask_vec, negative_vec):\n",
        "        ap_distance = tf.linalg.norm(wu_vec - mask_vec, axis=1)\n",
        "        an_distance = tf.linalg.norm(wu_vec - negative_vec, axis=1)\n",
        "        # ap_distance = 1-cossim(wu_vec , mask_vec)\n",
        "        # an_distance = 1-cossim(wu_vec , negative_vec)\n",
        "        # Computing the Triplet Loss by subtracting both distances and\n",
        "        # making sure we don't get a negative value.\n",
        "        # ap_distance = tf.reduce_sum(tf.square(wu_vec - mask_vec), 1)\n",
        "        # an_distance = tf.reduce_sum(tf.square(wu_vec - negative_vec), 1)\n",
        "        loss = ap_distance - an_distance\n",
        "        # print(loss)\n",
        "        loss = tf.maximum(loss + margin, 0.)\n",
        "        # tf.print(loss)\n",
        "        loss = tf.reduce_mean(loss)\n",
        "        return loss\n",
        "    return compute_loss\n",
        "\n",
        "def circle_loss(margin=0.01, gama=0.05):\n",
        "    def compute_loss(wu_vec, mask_vec, negative_vec):\n",
        "        sp = cossim(wu_vec , mask_vec)\n",
        "        sn = cossim(wu_vec , negative_vec)\n",
        "\n",
        "        loss = gama*tf.math.exp(sn - sp + margin, 0)\n",
        "        # loss = tf.math.log(1+ loss)\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "        return loss\n",
        "    return compute_loss\n",
        "\n",
        "def nce_loss(t=0.5):\n",
        "    def compute_loss(wu_vec, mask_vec, negative_vec):\n",
        "        sp = cossim(wu_vec , mask_vec)/t\n",
        "        sn = cossim(wu_vec , negative_vec)/t\n",
        "        loss = - tf.math.log( sp/(sp+sn) )\n",
        "        loss = tf.reduce_sum(loss)\n",
        "        return loss\n",
        "    return compute_loss\n",
        "\n",
        "\n",
        "# Compile model\n",
        "# model = Efficient_Rec( encoder = interaction_embedding2(), \n",
        "#                       reps = None, #get_reps_model(),\n",
        "#                       decoder = None ,# mask_label(),\n",
        "#                       use_tf_function=False)\n",
        "\n",
        "# # Configure the loss and optimizer\n",
        "# model.compile(\n",
        "#     optimizer=tf.optimizers.Adam(),\n",
        "#     loss= nce_loss(0.5)#constrastive_loss(margin= 1),\n",
        "# )"
      ],
      "metadata": {
        "id": "D5W_j9I7z5Qn"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# epochs= 5\n",
        "# test_user =warm_up_mask[warm_up_mask[userCol]<5000]#.sample(1000)\n",
        "# model_plot(model, movies, \n",
        "#                test_user)\n",
        "# for n in range(epochs):\n",
        "#   print(n, \"/\", epochs, \": \", model._constrastive_train_minibatch_step( train_warm.sample(n=5000), batch_size=512))\n",
        "#   model_plot(model, movies, \n",
        "#             test_user )\n",
        "#   gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOqPS8ISvA6d",
        "outputId": "9ac679d0-c44d-49a9-a741-8817e454a076"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 8.58 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# epochs= 20\n",
        "# for n in range(epochs):\n",
        "#   print(n, \"/\", epochs, \": \", model._constrastive_train_minibatch_step( interac_df, batch_size = 520 ))\n",
        "# gc.collect()"
      ],
      "metadata": {
        "id": "spjiPreaqSF9"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.layers[0].layers"
      ],
      "metadata": {
        "id": "-Zd0z6nPKrbQ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.expand_dims(np.roll(interac_df[\"movieId\"].apply(lambda x:' '.join([str(i) for i in x])), 1), -1)"
      ],
      "metadata": {
        "id": "_sq1qj0D4TY2"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# supervised constrastive "
      ],
      "metadata": {
        "id": "9wO4EX1_Z-h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SupervisedContrastiveLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, temperature=1, name=None):\n",
        "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
        "        # Normalize feature vectors\n",
        "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
        "        # Compute logits\n",
        "        logits = tf.divide(\n",
        "            tf.matmul(\n",
        "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
        "            ),\n",
        "            self.temperature,\n",
        "        )\n",
        "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)"
      ],
      "metadata": {
        "id": "hQ-pwUZafD__"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện training\n",
        "# TODO: sửa lại pd shift thành np.roll\n",
        "def _supervised_constrastive_train_step(self, inputs):\n",
        "    items_pd, ratings_pd, labels = inputs[\"movieId\"], inputs[\"y\"], inputs[\"label\"]\n",
        "    items, ratings = self._preprocess((items_pd, ratings_pd), wu_size)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Interaction embedding\n",
        "        vec = self.encoder([items, ratings])\n",
        "\n",
        "        average_loss = self.loss(labels, vec)\n",
        "\n",
        "    # Apply an optimization step\n",
        "    variables = self.trainable_variables \n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    # gradients = [None if gradient is None else tf.clip_by_value(gradient, -0.1, 0.1)\n",
        "    #              for gradient in gradients]\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "\n",
        "Efficient_Rec._supervised_constrastive_train_step = _supervised_constrastive_train_step"
      ],
      "metadata": {
        "id": "TbltflgfUmXf"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện minibatch training\n",
        "# def _spv_constrastive_train_minibatch_step(self, inputs, batch_size):\n",
        "#     # df1, df2, df3 = inputs[0].copy(), inputs[1].copy(), inputs[2].copy()\n",
        "#     get_copy = lambda inputs: (inputs[0].copy(), inputs[1].copy(), inputs[2].copy())\n",
        "#     get_chunk_df = lambda df:[df[i:i+batch_size] for i in range(0,df.shape[0],batch_size)]\n",
        "#     get_chunk = lambda inputs: [get_chunk_df(df) for df in inputs]\n",
        "#     copies = get_copy( inputs )\n",
        "#     chunks = get_chunk(copies)\n",
        "\n",
        "#     get_chunk_i =  lambda chunks, i: (chunk[i] for chunk in chunks)\n",
        "#     losses = []\n",
        "#     for i in range(math.ceil(inputs[0].shape[0]/batch_size)):\n",
        "#         chunk = get_chunk_i(chunks, i)\n",
        "#         loss = self._supervised_constrastive_train_step(chunk)\n",
        "#         losses.append(loss[\"batch_loss\"].numpy())\n",
        "#         print(loss)\n",
        "#     return np.mean(losses)\n",
        "\n",
        "# Efficient_Rec._spv_constrastive_train_minibatch_step = _spv_constrastive_train_minibatch_step"
      ],
      "metadata": {
        "id": "aPG0aQ3vO4_n"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện minibatch training\n",
        "def _spv_constrastive_train_minibatch_step(self, inputs, batch_size):\n",
        "    df = inputs.copy()\n",
        "    chunks = [df[i:i+batch_size] for i in range(0,df.shape[0],batch_size)]\n",
        "    losses = []\n",
        "    for chunk in chunks:\n",
        "        loss = self._supervised_constrastive_train_step(chunk)\n",
        "        losses.append(loss[\"batch_loss\"].numpy())\n",
        "        print(loss)\n",
        "        gc.collect()\n",
        "    return np.mean(losses)\n",
        "\n",
        "Efficient_Rec._spv_constrastive_train_minibatch_step = _spv_constrastive_train_minibatch_step"
      ],
      "metadata": {
        "id": "qSr05iptaPha"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model = Efficient_Rec( encoder = interaction_embedding2(), \n",
        "                      reps = None, #get_reps_model(),\n",
        "                      decoder = None ,# mask_label(),\n",
        "                      use_tf_function=False)\n",
        "model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate = 0.001),\n",
        "    loss= TripletSemiHardLoss()#SupervisedContrastiveLoss(temperature = 0.05),\n",
        ")"
      ],
      "metadata": {
        "id": "uBCoJlJMdB7y"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train[\"movieId\"].apply(lambda x: int(len(x)/10)*10).to_frame(0).reset_index().groupby(0).count(\n",
        "# ).sort_values(\"userId\", ascending=False)"
      ],
      "metadata": {
        "id": "CJrj-BPMQ2lL"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start_u = 5000\n",
        "# test_set = warm_up_mask[(warm_up_mask[userCol]>start_u)&(warm_up_mask[userCol]<(start_u+5000))]\n",
        "# model_plot(model, movies, test_set )"
      ],
      "metadata": {
        "id": "V4jF7Vw7-UQU"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# epochs= 5\n",
        "# test_set = warm_up_mask[(warm_up_mask[userCol]>10000)&(warm_up_mask[userCol]<15000)]\n",
        "# model_plot(model, movies, test_set )\n",
        "# for n in range(epochs):\n",
        "#   print(n, \"/\", epochs, \": \", model._spv_constrastive_train_minibatch_step(train_warm.sample(frac=0.1), batch_size=512))\n",
        "#   model_plot(model, movies, test_set )\n",
        "#   gc.collect()"
      ],
      "metadata": {
        "id": "VoepslyXhaQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a25c7da-d458-4716-9640-fa1aeec72f22"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
            "Wall time: 8.82 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_evaluate(model, movies, \n",
        "#                warm_up_mask[warm_up_mask[userCol]<5000])"
      ],
      "metadata": {
        "id": "BKXC3Y6tRgt2"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_evaluate(model, movies, \n",
        "#                warm_up_mask[(warm_up_mask[userCol]>u_train_to)&(warm_up_mask[userCol]<(u_train_to+5000))])"
      ],
      "metadata": {
        "id": "ZZUCY20TSjFk"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Triplet loss\n",
        "Triplet loss với warm up, mask lấy từ interaction cùng 1 người, negative chọn từ một người khác không cùng category"
      ],
      "metadata": {
        "id": "8UgNHOHFtLg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: chuyển note sau thành file .py"
      ],
      "metadata": {
        "id": "Nj6qqxfAtrtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # WU-Mask triplet loss\n",
        "# import tensorflow as tf\n",
        "# from tensorflow_addons.losses import metric_learning\n",
        "# from tensorflow_addons.utils.keras_utils import LossFunctionWrapper\n",
        "# from tensorflow_addons.utils.types import FloatTensorLike, TensorLike\n",
        "# from typeguard import typechecked\n",
        "# from typing import Optional, Union, Callable\n",
        "\n",
        "\n",
        "# def _masked_minimum(data, mask, dim=1):\n",
        "#     \"\"\"Computes the axis wise minimum over chosen elements.\n",
        "#     Args:\n",
        "#       data: 2-D float `Tensor` of size [n, m].\n",
        "#       mask: 2-D Boolean `Tensor` of size [n, m].\n",
        "#       dim: The dimension over which to compute the minimum.\n",
        "#     Returns:\n",
        "#       masked_minimums: N-D `Tensor`.\n",
        "#         The minimized dimension is of size 1 after the operation.\n",
        "#     \"\"\"\n",
        "#     axis_maximums = tf.math.reduce_max(data, dim, keepdims=True)\n",
        "#     masked_minimums = (\n",
        "#         tf.math.reduce_min(\n",
        "#             tf.math.multiply(data - axis_maximums, mask), dim, keepdims=True\n",
        "#         )\n",
        "#         + axis_maximums\n",
        "#     )\n",
        "#     return masked_minimums\n",
        "\n",
        "\n",
        "# @tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
        "# @tf.function\n",
        "# def triplet_hard_warmup_mask_loss(\n",
        "#     y_true: TensorLike,\n",
        "#     y_pred: TensorLike,\n",
        "#     margin: FloatTensorLike = 1.0,\n",
        "#     soft: bool = False,\n",
        "#     distance_metric: Union[str, Callable] = \"L2\",\n",
        "# ) -> tf.Tensor:\n",
        "#     \"\"\"Computes the triplet loss with hard negative and hard positive mining.\n",
        "#     Args:\n",
        "#       y_true: 1-D integer `Tensor` with shape [batch_size] of\n",
        "#         multiclass integer labels.\n",
        "#       y_pred: 2-D float `Tensor` of embedding vectors. Embeddings should\n",
        "#         be l2 normalized.\n",
        "#       margin: Float, margin term in the loss definition.\n",
        "#       soft: Boolean, if set, use the soft margin version.\n",
        "#       distance_metric: str or function, determines distance metric:\n",
        "#                        \"L2\" for l2-norm distance\n",
        "#                        \"squared-L2\" for squared l2-norm distance\n",
        "#                        \"angular\" for cosine similarity\n",
        "#                         A custom function returning a 2d adjacency\n",
        "#                           matrix of a chosen distance metric can\n",
        "#                           also be passed here. e.g.\n",
        "#                           def custom_distance(batch):\n",
        "#                               batch = 1 - batch @ batch.T\n",
        "#                               return batch\n",
        "#                           triplet_semihard_loss(batch, labels,\n",
        "#                                         distance_metric=custom_distance\n",
        "#                                     )\n",
        "#     Returns:\n",
        "#       triplet_loss: float scalar with dtype of y_pred.\n",
        "#     \"\"\"\n",
        "\n",
        "#     labels, embeddings = y_true, y_pred\n",
        "#     wu_embeddings, mask_embeddings = embeddings\n",
        "\n",
        "#     def convert_to_float(embeddings):\n",
        "#         convert_to_float32 = (\n",
        "#             embeddings.dtype == tf.dtypes.float16 or embeddings.dtype == tf.dtypes.bfloat16\n",
        "#         )\n",
        "#         precise_wu_embeddings = (\n",
        "#             tf.cast(embeddings, tf.dtypes.float32) if convert_to_float32 else embeddings\n",
        "#         )\n",
        "#         return precise_wu_embeddings, convert_to_float32\n",
        "\n",
        "#     precise_wu_embeddings, convert_to_float32_wu = convert_to_float(wu_embeddings)\n",
        "#     precise_mask_embeddings, convert_to_float32_mask = convert_to_float(mask_embeddings)\n",
        "\n",
        "#     convert_to_float32 = (convert_to_float32_wu or convert_to_float32_mask)\n",
        "\n",
        "#     # Reshape label tensor to [batch_size, 1].\n",
        "#     lshape = tf.shape(labels)\n",
        "#     labels = tf.reshape(labels, [lshape[0], 1])\n",
        "\n",
        "#     # Build pairwise squared distance matrix.\n",
        "#     if distance_metric == \"L2\":\n",
        "#         pdist_matrix = metric_learning.pairwise_distance(\n",
        "#             precise_wu_embeddings, squared=False\n",
        "#         )\n",
        "#         hard_positives =  metric_learning.pairwise_distance(\n",
        "#             precise_wu_embeddings - precise_mask_embeddings, squared=False\n",
        "#         )\n",
        "\n",
        "#     elif distance_metric == \"squared-L2\":\n",
        "#         pdist_matrix = metric_learning.pairwise_distance(\n",
        "#             precise_wu_embeddings, squared=True\n",
        "#         )\n",
        "#         hard_positives =  metric_learning.pairwise_distance(\n",
        "#             precise_wu_embeddings - precise_mask_embeddings, squared=True\n",
        "#         )\n",
        "\n",
        "#     elif distance_metric == \"angular\":\n",
        "#         pdist_matrix = metric_learning.angular_distance(precise_wu_embeddings)\n",
        "#         hard_positives =  metric_learning.angular_distance(\n",
        "#             precise_wu_embeddings - precise_mask_embeddings\n",
        "#         )\n",
        "\n",
        "#     else:\n",
        "#         pdist_matrix = distance_metric(precise_wu_embeddings)\n",
        "#         hard_positives =  distance_metric( precise_wu_embeddings - precise_mask_embeddings )\n",
        "    \n",
        "\n",
        "#     # Build pairwise binary adjacency matrix.\n",
        "#     adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
        "#     # Invert so we can select negatives only.\n",
        "#     adjacency_not = tf.math.logical_not(adjacency)\n",
        "\n",
        "#     adjacency_not = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n",
        "#     # hard negatives: smallest D_an.\n",
        "#     hard_negatives = _masked_minimum(pdist_matrix, adjacency_not)\n",
        "\n",
        "#     hard_positives = tf.expand_dims(hard_positives, -1)\n",
        "    \n",
        "\n",
        "#     if soft:\n",
        "#         triplet_loss = tf.math.log1p(tf.math.exp(hard_positives - hard_negatives))\n",
        "#     else:\n",
        "#         triplet_loss = tf.maximum(hard_positives - hard_negatives + margin, 0.0)\n",
        "\n",
        "#     # Get final mean triplet loss\n",
        "#     triplet_loss = tf.reduce_mean(triplet_loss)\n",
        "\n",
        "#     if convert_to_float32:\n",
        "#         return tf.cast(triplet_loss, embeddings.dtype)\n",
        "#     else:\n",
        "#         return triplet_loss\n",
        "\n",
        "\n",
        "\n",
        "# @tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
        "# class TripletHardWarmupMaskedLoss(LossFunctionWrapper):\n",
        "#     \"\"\"Computes the triplet loss with hard negative and hard positive mining.\n",
        "#     The loss encourages the maximum positive distance (between a pair of embeddings\n",
        "#     with the same labels) to be smaller than the minimum negative distance plus the\n",
        "#     margin constant in the mini-batch.\n",
        "#     The loss selects the hardest positive and the hardest negative samples\n",
        "#     within the batch when forming the triplets for computing the loss.\n",
        "#     See: https://arxiv.org/pdf/1703.07737.\n",
        "#     We expect labels `y_true` to be provided as 1-D integer `Tensor` with shape\n",
        "#     [batch_size] of multi-class integer labels. And embeddings `y_pred` must be\n",
        "#     2-D float `Tensor` of l2 normalized embedding vectors.\n",
        "#     Args:\n",
        "#       margin: Float, margin term in the loss definition. Default value is 1.0.\n",
        "#       soft: Boolean, if set, use the soft margin version. Default value is False.\n",
        "#       name: Optional name for the op.\n",
        "#     \"\"\"\n",
        "\n",
        "#     @typechecked\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         margin: FloatTensorLike = 1.0,\n",
        "#         soft: bool = False,\n",
        "#         distance_metric: Union[str, Callable] = \"L2\",\n",
        "#         name: Optional[str] = None,\n",
        "#         **kwargs\n",
        "#     ):\n",
        "#         super().__init__(\n",
        "#             triplet_hard_warmup_mask_loss,\n",
        "#             name=name,\n",
        "#             reduction=tf.keras.losses.Reduction.NONE,\n",
        "#             margin=margin,\n",
        "#             soft=soft,\n",
        "#             distance_metric=distance_metric,\n",
        "#         )"
      ],
      "metadata": {
        "id": "t8vcPXVTtoUi"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Triplet ver1 với sửa lại với negative sample: so sánh khoảng cách của toàn bộ tương tác của người dùng ancol với negative (thay vì warm-up ancol với warm-up negative), với positive giữ nguyên (so sánh warm up và mask)"
      ],
      "metadata": {
        "id": "vkNRLv6h-Xg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # WU-Mask triplet loss\n",
        "# import tensorflow as tf\n",
        "# from tensorflow_addons.losses import metric_learning\n",
        "# from tensorflow_addons.utils.keras_utils import LossFunctionWrapper\n",
        "# from tensorflow_addons.utils.types import FloatTensorLike, TensorLike\n",
        "# from typeguard import typechecked\n",
        "# from typing import Optional, Union, Callable\n",
        "\n",
        "\n",
        "# def _masked_minimum(data, mask, dim=1):\n",
        "#     \"\"\"Computes the axis wise minimum over chosen elements.\n",
        "#     Args:\n",
        "#       data: 2-D float `Tensor` of size [n, m].\n",
        "#       mask: 2-D Boolean `Tensor` of size [n, m].\n",
        "#       dim: The dimension over which to compute the minimum.\n",
        "#     Returns:\n",
        "#       masked_minimums: N-D `Tensor`.\n",
        "#         The minimized dimension is of size 1 after the operation.\n",
        "#     \"\"\"\n",
        "#     axis_maximums = tf.math.reduce_max(data, dim, keepdims=True)\n",
        "#     masked_minimums = (\n",
        "#         tf.math.reduce_min(\n",
        "#             tf.math.multiply(data - axis_maximums, mask), dim, keepdims=True\n",
        "#         )\n",
        "#         + axis_maximums\n",
        "#     )\n",
        "#     return masked_minimums\n",
        "\n",
        "\n",
        "# @tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
        "# @tf.function\n",
        "# def triplet_hard_warmup_mask_loss2(\n",
        "#     y_true: TensorLike,\n",
        "#     y_pred: TensorLike,\n",
        "#     margin: FloatTensorLike = 1.0,\n",
        "#     soft: bool = False,\n",
        "#     distance_metric: Union[str, Callable] = \"L2\",\n",
        "# ) -> tf.Tensor:\n",
        "#     \"\"\"Computes the triplet loss with hard negative and hard positive mining.\n",
        "#     Args:\n",
        "#       y_true: 1-D integer `Tensor` with shape [batch_size] of\n",
        "#         multiclass integer labels.\n",
        "#       y_pred: 2-D float `Tensor` of embedding vectors. Embeddings should\n",
        "#         be l2 normalized.\n",
        "#       margin: Float, margin term in the loss definition.\n",
        "#       soft: Boolean, if set, use the soft margin version.\n",
        "#       distance_metric: str or function, determines distance metric:\n",
        "#                        \"L2\" for l2-norm distance\n",
        "#                        \"squared-L2\" for squared l2-norm distance\n",
        "#                        \"angular\" for cosine similarity\n",
        "#                         A custom function returning a 2d adjacency\n",
        "#                           matrix of a chosen distance metric can\n",
        "#                           also be passed here. e.g.\n",
        "#                           def custom_distance(batch):\n",
        "#                               batch = 1 - batch @ batch.T\n",
        "#                               return batch\n",
        "#                           triplet_semihard_loss(batch, labels,\n",
        "#                                         distance_metric=custom_distance\n",
        "#                                     )\n",
        "#     Returns:\n",
        "#       triplet_loss: float scalar with dtype of y_pred.\n",
        "#     \"\"\"\n",
        "\n",
        "#     labels, embeddings = y_true, y_pred\n",
        "#     wu_embeddings, mask_embeddings = embeddings\n",
        "\n",
        "#     def convert_to_float(embeddings):\n",
        "#         convert_to_float32 = (\n",
        "#             embeddings.dtype == tf.dtypes.float16 or embeddings.dtype == tf.dtypes.bfloat16\n",
        "#         )\n",
        "#         precise_wu_embeddings = (\n",
        "#             tf.cast(embeddings, tf.dtypes.float32) if convert_to_float32 else embeddings\n",
        "#         )\n",
        "#         return precise_wu_embeddings, convert_to_float32\n",
        "\n",
        "#     precise_wu_embeddings, convert_to_float32_wu = convert_to_float(wu_embeddings)\n",
        "#     precise_mask_embeddings, convert_to_float32_mask = convert_to_float(mask_embeddings)\n",
        "#     precise_embeddings = tf.math.add(precise_wu_embeddings, precise_mask_embeddings)\n",
        "\n",
        "#     convert_to_float32 = (convert_to_float32_wu or convert_to_float32_mask)\n",
        "\n",
        "#     # Reshape label tensor to [batch_size, 1].\n",
        "#     lshape = tf.shape(labels)\n",
        "#     labels = tf.reshape(labels, [lshape[0], 1])\n",
        "\n",
        "#     # Build pairwise squared distance matrix.\n",
        "#     if distance_metric == \"L2\":\n",
        "#         pdist_matrix = metric_learning.pairwise_distance(\n",
        "#             precise_embeddings, squared=False\n",
        "#         )\n",
        "#         hard_positives =  metric_learning.pairwise_distance(\n",
        "#             precise_wu_embeddings - precise_mask_embeddings, squared=False\n",
        "#         )\n",
        "\n",
        "#     elif distance_metric == \"squared-L2\":\n",
        "#         pdist_matrix = metric_learning.pairwise_distance(\n",
        "#             precise_embeddings, squared=True\n",
        "#         )\n",
        "#         hard_positives =  metric_learning.pairwise_distance(\n",
        "#             precise_wu_embeddings - precise_mask_embeddings, squared=True\n",
        "#         )\n",
        "\n",
        "#     elif distance_metric == \"angular\":\n",
        "#         pdist_matrix = metric_learning.angular_distance(precise_embeddings)\n",
        "#         hard_positives =  metric_learning.angular_distance(\n",
        "#             precise_wu_embeddings - precise_mask_embeddings\n",
        "#         )\n",
        "\n",
        "#     else:\n",
        "#         pdist_matrix = distance_metric(precise_embeddings)\n",
        "#         hard_positives =  distance_metric( precise_wu_embeddings - precise_mask_embeddings )\n",
        "    \n",
        "\n",
        "#     # Build pairwise binary adjacency matrix.\n",
        "#     adjacency = tf.math.equal(labels, tf.transpose(labels))\n",
        "#     # Invert so we can select negatives only.\n",
        "#     adjacency_not = tf.math.logical_not(adjacency)\n",
        "\n",
        "#     adjacency_not = tf.cast(adjacency_not, dtype=tf.dtypes.float32)\n",
        "#     # hard negatives: smallest D_an.\n",
        "#     hard_negatives = _masked_minimum(pdist_matrix, adjacency_not)\n",
        "\n",
        "#     hard_positives = tf.expand_dims(hard_positives, -1)\n",
        "    \n",
        "\n",
        "#     if soft:\n",
        "#         triplet_loss = tf.math.log1p(tf.math.exp(hard_positives - hard_negatives))\n",
        "#     else:\n",
        "#         triplet_loss = tf.maximum(hard_positives - hard_negatives + margin, 0.0)\n",
        "\n",
        "#     # Get final mean triplet loss\n",
        "#     triplet_loss = tf.reduce_mean(triplet_loss)\n",
        "\n",
        "#     if convert_to_float32:\n",
        "#         return tf.cast(triplet_loss, embeddings.dtype)\n",
        "#     else:\n",
        "#         return triplet_loss\n",
        "\n",
        "\n",
        "\n",
        "# @tf.keras.utils.register_keras_serializable(package=\"Addons\")\n",
        "# class TripletHardWarmupMaskedLoss2(LossFunctionWrapper):\n",
        "#     \"\"\"Computes the triplet loss with hard negative and hard positive mining.\n",
        "#     The loss encourages the maximum positive distance (between a pair of embeddings\n",
        "#     with the same labels) to be smaller than the minimum negative distance plus the\n",
        "#     margin constant in the mini-batch.\n",
        "#     The loss selects the hardest positive and the hardest negative samples\n",
        "#     within the batch when forming the triplets for computing the loss.\n",
        "#     See: https://arxiv.org/pdf/1703.07737.\n",
        "#     We expect labels `y_true` to be provided as 1-D integer `Tensor` with shape\n",
        "#     [batch_size] of multi-class integer labels. And embeddings `y_pred` must be\n",
        "#     2-D float `Tensor` of l2 normalized embedding vectors.\n",
        "#     Args:\n",
        "#       margin: Float, margin term in the loss definition. Default value is 1.0.\n",
        "#       soft: Boolean, if set, use the soft margin version. Default value is False.\n",
        "#       name: Optional name for the op.\n",
        "#     \"\"\"\n",
        "\n",
        "#     @typechecked\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         margin: FloatTensorLike = 1.0,\n",
        "#         soft: bool = False,\n",
        "#         distance_metric: Union[str, Callable] = \"L2\",\n",
        "#         name: Optional[str] = None,\n",
        "#         **kwargs\n",
        "#     ):\n",
        "#         super().__init__(\n",
        "#             triplet_hard_warmup_mask_loss2,\n",
        "#             name=name,\n",
        "#             reduction=tf.keras.losses.Reduction.NONE,\n",
        "#             margin=margin,\n",
        "#             soft=soft,\n",
        "#             distance_metric=distance_metric,\n",
        "#         )"
      ],
      "metadata": {
        "id": "sc5GuC6--WBr"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện training\n",
        "def _wu_mask_triplet_train_step(self, inputs):\n",
        "    items_pd, ratings_pd, labels = inputs[\"movieId\"], inputs[\"y\"], inputs[\"label\"]\n",
        "    warm_up, mask = self._wu_mask_split2(inputs.sample(frac=1), mask_ratio = 0.5)\n",
        "    wu_items, wu_ratings = self._preprocess(warm_up, wu_size)\n",
        "    mask_item, mask_ratings = self._preprocess(mask, wu_size)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Interaction embedding\n",
        "        wu_vec = self.encoder([wu_items, wu_ratings])\n",
        "        mask_vec = self.encoder([mask_item, mask_ratings])\n",
        "\n",
        "        average_loss = self.loss( labels,  (wu_vec, mask_vec)) \n",
        "\n",
        "    # Apply an optimization step\n",
        "    variables = self.trainable_variables \n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    # gradients = [None if gradient is None else tf.clip_by_value(gradient, -0.1, 0.1)\n",
        "    #              for gradient in gradients]\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "\n",
        "Efficient_Rec._wu_mask_triplet_train_step = _wu_mask_triplet_train_step"
      ],
      "metadata": {
        "id": "jJ2lvQx-ukRu"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện minibatch training\n",
        "def _wu_mask_triplet_train_minibatch_step(self, inputs, batch_size):\n",
        "    df = inputs.copy()\n",
        "    chunks = [df[i:i+batch_size] for i in range(0,df.shape[0],batch_size)]\n",
        "    losses = []\n",
        "    for chunk in chunks:\n",
        "        loss = self._wu_mask_triplet_train_step(chunk)\n",
        "        losses.append(loss[\"batch_loss\"].numpy())\n",
        "        print(loss)\n",
        "        gc.collect()\n",
        "    return np.mean(losses)\n",
        "\n",
        "Efficient_Rec._wu_mask_triplet_train_minibatch_step = _wu_mask_triplet_train_minibatch_step"
      ],
      "metadata": {
        "id": "ubXjhvrgxfbo"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model = Efficient_Rec( encoder = interaction_embedding2(), \n",
        "                      use_tf_function=False)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=TripletHardWarmupMaskedLoss2())"
      ],
      "metadata": {
        "id": "eOjkNyaZtgBM"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "epochs= 1\n",
        "# test_set = warm_up_mask[(warm_up_mask[userCol]>1000)&(warm_up_mask[userCol]<2000)]\n",
        "# model_plot( model, movies, test_set )\n",
        "# for n in range(epochs):\n",
        "#   print(n, \"/\", epochs, \": \", model._wu_mask_triplet_train_minibatch_step(train_warm.sample(frac=0.1), batch_size=512))\n",
        "#   model_plot(model, movies, test_set )\n",
        "#   gc.collect()"
      ],
      "metadata": {
        "id": "nPrQu5yGtpDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "294a1250-46d7-422c-e15e-3a548307ccad"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 7.39 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "WSAWNkXjGX7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0090de4a-5e9f-49d6-950e-e1a05ad9a23d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_evaluate(model, movies, \n",
        "#                warm_up_mask[(warm_up_mask[userCol]>u_train_to)&(warm_up_mask[userCol]<(u_train_to+5000))])"
      ],
      "metadata": {
        "id": "WGxSqovoy8UD"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pick item pipeline"
      ],
      "metadata": {
        "id": "CgfpZRoGF1KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_shortlist(ratings[ratings[userCol]<=3000], limit = 1000, cluster_num = 5)"
      ],
      "metadata": {
        "id": "fQtrddWcEaoY"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "interaction_list = train_warm[train_warm.index<=3000]\n",
        "top_k = 50\n",
        "is_remove_interacted = True\n",
        "\n",
        "y_pred = model.get_recommendation(interaction_list, top_k = top_k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1MUU9bdHQAP",
        "outputId": "db1264e9-3c47-4c3e-a4ab-d002786810df"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3min 10s, sys: 6.24 s, total: 3min 16s\n",
            "Wall time: 3min 5s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = ratings[(ratings[\"y\"]>0)&(ratings[userCol]<=3000)][[userCol, itemCol, \"y\"]]"
      ],
      "metadata": {
        "id": "2dGbwAJOur5Y"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rs(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    y_true: dataframe: user_id, item_id, y (rating normailised), only favorite item\n",
        "    y_pred: dataframe: user_id, item_id (just top k item)\n",
        "    return:\n",
        "    precision@k, recall@k\n",
        "    \"\"\"\n",
        "    total1 = y_true.merge(y_pred, on=[userCol, itemCol], how = 'outer', suffixes=('_t', '_p'))\n",
        "    total1['is_pt'] = total1.apply(lambda x: 0 if (np.isnan(x[\"y\"]) or np.isnan(x[\"rank\"]) ) else 1,axis=1)\n",
        "    total = total1.groupby(userCol).agg({\n",
        "        \"y\":'count',\n",
        "        \"rank\":'count',\n",
        "        \"is_pt\": 'sum'\n",
        "        })\n",
        "    total.columns = [\"true_num\", \"predict_num\", \"pred_true_num\"]\n",
        "    total[\"macro_p\"] = total[\"pred_true_num\"]/total[\"predict_num\"]\n",
        "    total[\"macro_r\"] = total[\"pred_true_num\"]/total[\"true_num\"]\n",
        "\n",
        "    macro_p = total[total[\"predict_num\"]>0][\"macro_p\"].mean()\n",
        "    macro_r = total[total[\"true_num\"]>0][\"macro_r\"].mean()\n",
        "\n",
        "    micro_p = total[\"pred_true_num\"].sum()/total[\"predict_num\"].sum()\n",
        "    micro_r = total[\"pred_true_num\"].sum()/total[\"true_num\"].sum()\n",
        "\n",
        "    print(\"macro_p: \", macro_p, \"; macro_r :\", macro_r)\n",
        "    print(\"micro_p: \", micro_p, \"; micro_r :\", micro_r)\n",
        "\n",
        "    return macro_p, macro_r, micro_p, micro_r"
      ],
      "metadata": {
        "id": "QPPnAXS9vBhG"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "macro_p, macro_r, micro_p, micro_r= evaluate_rs(y_true, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM00AWaQ1H1g",
        "outputId": "23b6372d-6f24-44ec-a6a2-51b14f0411aa"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "macro_p:  0.0004864091559370529 ; macro_r : 1.5957011522252546e-05\n",
            "micro_p:  0.0004864091559370529 ; micro_r : 4.665932925841858e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "PxW4a3ZSUf0h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "c4da9a74-ace9-4b99-e86d-b2aad6f0aa94"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        userId  movieId  rank\n",
              "2            1      134  13.0\n",
              "171          1     3218  37.0\n",
              "262          1     5104  35.0\n",
              "280          1     5351   3.0\n",
              "359          1     6465   4.0\n",
              "...        ...      ...   ...\n",
              "743002    2995   107230  19.0\n",
              "743015    2995   111913  30.0\n",
              "743016    2995   111931  42.0\n",
              "743019    2995   112334  50.0\n",
              "743030    2995   116161  38.0\n",
              "\n",
              "[34950 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f93bdd8-577c-4142-ba81-361c4831535c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>134</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>1</td>\n",
              "      <td>3218</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>1</td>\n",
              "      <td>5104</td>\n",
              "      <td>35.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>1</td>\n",
              "      <td>5351</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>1</td>\n",
              "      <td>6465</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743002</th>\n",
              "      <td>2995</td>\n",
              "      <td>107230</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743015</th>\n",
              "      <td>2995</td>\n",
              "      <td>111913</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743016</th>\n",
              "      <td>2995</td>\n",
              "      <td>111931</td>\n",
              "      <td>42.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743019</th>\n",
              "      <td>2995</td>\n",
              "      <td>112334</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743030</th>\n",
              "      <td>2995</td>\n",
              "      <td>116161</td>\n",
              "      <td>38.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>34950 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f93bdd8-577c-4142-ba81-361c4831535c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f93bdd8-577c-4142-ba81-361c4831535c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f93bdd8-577c-4142-ba81-361c4831535c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total1 = y_true.merge(y_pred, on=[userCol, itemCol], how = 'outer', suffixes=('_t', '_p'))\n",
        "total1['is_pt'] = total1.apply(lambda x: 0 if (np.isnan(x[\"y\"]) or np.isnan(x[\"rank\"]) ) else 1,axis=1)\n",
        "total = total1.groupby(userCol).agg({\n",
        "    \"y\":'count',\n",
        "    \"rank\":'count',\n",
        "    \"is_pt\": 'sum'\n",
        "    })\n",
        "total.columns = [\"true_num\", \"predict_num\", \"pred_true_num\"]\n",
        "total[\"macro_p\"] = total[\"pred_true_num\"]/total[\"predict_num\"]\n",
        "total[\"macro_r\"] = total[\"pred_true_num\"]/total[\"true_num\"]\n",
        "\n",
        "macro_p = total[total[\"predict_num\"]>0][\"macro_p\"].mean()\n",
        "macro_r = total[total[\"true_num\"]>0][\"macro_r\"].mean()\n",
        "\n",
        "micro_p = total[\"pred_true_num\"].sum()/total[\"predict_num\"].sum()\n",
        "micro_r = total[\"pred_true_num\"].sum()/total[\"true_num\"].sum()\n",
        "\n",
        "print(\"macro_p: \", macro_p, \"; macro_r :\", macro_r)\n",
        "print(\"micro_p: \", micro_p, \"; micro_r :\", micro_r)"
      ],
      "metadata": {
        "id": "jZsyQTl82HYE",
        "outputId": "58eb04c7-788e-4e1f-abd3-fca289f0864e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "macro_p:  0.0004864091559370529 ; macro_r : 1.5957011522252546e-05\n",
            "micro_p:  0.0004864091559370529 ; micro_r : 4.665932925841858e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END"
      ],
      "metadata": {
        "id": "lHXHyIUJy6Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "4BbEPlHH31zA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "v2.2_ML20M_sequence.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "17fHxkHoAQq6"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}