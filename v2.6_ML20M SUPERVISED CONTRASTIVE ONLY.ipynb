{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quanvu0996/TF_cert_training/blob/main/v2.6_ML20M%20SUPERVISED%20CONTRASTIVE%20ONLY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ver 6.0 \n",
        "Supervised contrastive"
      ],
      "metadata": {
        "id": "a8qpAznwl3Fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons\n",
        "# !pip install recommenders\n",
        "!pip install dask[dataframe] --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7sPzaTKcdTo",
        "outputId": "48696f68-10c4-4059-fc24-aad9d4345556"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.17.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.7/dist-packages (2.12.0)\n",
            "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (1.21.6)\n",
            "Collecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]) (0.11.2)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[dataframe]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->dask[dataframe]) (2022.1)\n",
            "Collecting locket\n",
            "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.0->dask[dataframe]) (1.15.0)\n",
            "Installing collected packages: locket, partd, fsspec\n",
            "Successfully installed fsspec-2022.5.0 locket-1.0.0 partd-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libs"
      ],
      "metadata": {
        "id": "oGKXb_a2GGDk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XS6UHXPVAQqs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "import gc\n",
        "import math\n",
        "import datetime, time\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "\n",
        "import dask.dataframe as dd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.losses import TripletSemiHardLoss \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from recommenders.datasets.python_splitters import python_random_split\n",
        "from recommenders.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\n",
        "from recommenders.models.cornac.cornac_utils import predict_ranking"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Gv8gXemwD3UQ",
        "outputId": "d1c98c0e-a334-4a75-d7a0-8c7a0b5b258a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Crhty6fiAQqu"
      },
      "outputs": [],
      "source": [
        "itemCol = 'movieId'\n",
        "userCol = 'userId'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DGX setup\n",
        "# fpath = \"./ml-20m\" \n",
        "\n",
        "#colab setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "fpath = \"/content/gdrive/MyDrive/RECOMMENDER_STUDIES/data/ml-20m\""
      ],
      "metadata": {
        "id": "xOJJy441A0w0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b47e7c7-3360-4f3f-f1d2-38349e74f114"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading data\n",
        "ratings = dd.read_csv(fpath+'/ratings.csv')\n",
        "\n",
        "ratings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "jQck8YHxCPJ1",
        "outputId": "53f3f58b-bfea-4997-c97b-5932a405e8bd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dask DataFrame Structure:\n",
              "              userId movieId   rating timestamp\n",
              "npartitions=9                                  \n",
              "               int64   int64  float64     int64\n",
              "                 ...     ...      ...       ...\n",
              "...              ...     ...      ...       ...\n",
              "                 ...     ...      ...       ...\n",
              "                 ...     ...      ...       ...\n",
              "Dask Name: from-delayed, 27 tasks"
            ],
            "text/html": [
              "<div><strong>Dask DataFrame Structure:</strong></div>\n",
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userId</th>\n",
              "      <th>movieId</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>npartitions=9</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>int64</td>\n",
              "      <td>int64</td>\n",
              "      <td>float64</td>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "<div>Dask Name: from-delayed, 27 tasks</div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize rating về dạng -1 -> 1 (-1 = ghét, 1 = thích)\n",
        "ratings[\"y\"] = ratings[\"rating\"]/2.5-1\n",
        "\n",
        "# Kiểm tra rating trong khoảng -1 -> 1\n",
        "# ratings.groupby(\"y\")[\"y\"].count().plot(kind='bar')"
      ],
      "metadata": {
        "id": "BrwYHetxCiIw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading movie\n",
        "movies = dd.read_csv(fpath+'/movies.csv')\n",
        "movies[\"year\"]=movies[\"title\"].apply(lambda x: x[-5:-1])\n",
        "movies[\"genres\"] = movies[\"genres\"].apply(lambda x: ' ' if x == '(no genres listed)' else ' '.join(x.split('|')) )\n",
        "movies[\"title\"]= movies[\"title\"].apply(lambda x: x[0:-7])\n",
        "movies.head()"
      ],
      "metadata": {
        "id": "_DH8SxPH64nZ",
        "outputId": "1e4603f9-90a3-4ff2-827d-a12317988595",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/dask/dataframe/core.py:3073: UserWarning: \n",
            "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
            "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
            "  Before: .apply(func)\n",
            "  After:  .apply(func, meta=('title', 'object'))\n",
            "\n",
            "  warnings.warn(meta_warning(meta))\n",
            "/usr/local/lib/python3.7/dist-packages/dask/dataframe/core.py:3073: UserWarning: \n",
            "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
            "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
            "  Before: .apply(func)\n",
            "  After:  .apply(func, meta=('genres', 'object'))\n",
            "\n",
            "  warnings.warn(meta_warning(meta))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   movieId                        title  \\\n",
              "0        1                    Toy Story   \n",
              "1        2                      Jumanji   \n",
              "2        3             Grumpier Old Men   \n",
              "3        4            Waiting to Exhale   \n",
              "4        5  Father of the Bride Part II   \n",
              "\n",
              "                                        genres  year  \n",
              "0  Adventure Animation Children Comedy Fantasy  1995  \n",
              "1                   Adventure Children Fantasy  1995  \n",
              "2                               Comedy Romance  1995  \n",
              "3                         Comedy Drama Romance  1995  \n",
              "4                                       Comedy  1995  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59f22253-81fb-4a01-b89c-ebeee2b12d6f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movieId</th>\n",
              "      <th>title</th>\n",
              "      <th>genres</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Toy Story</td>\n",
              "      <td>Adventure Animation Children Comedy Fantasy</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Jumanji</td>\n",
              "      <td>Adventure Children Fantasy</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Grumpier Old Men</td>\n",
              "      <td>Comedy Romance</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Waiting to Exhale</td>\n",
              "      <td>Comedy Drama Romance</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Father of the Bride Part II</td>\n",
              "      <td>Comedy</td>\n",
              "      <td>1995</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59f22253-81fb-4a01-b89c-ebeee2b12d6f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-59f22253-81fb-4a01-b89c-ebeee2b12d6f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-59f22253-81fb-4a01-b89c-ebeee2b12d6f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train-test split\n",
        "fulltrain, test= train_test_split(ratings, test_size= 0.25)\n",
        "train = fulltrain[fulltrain[userCol]<=50000]"
      ],
      "metadata": {
        "id": "fiRtTNy1ScIa",
        "outputId": "0a7898e4-7eec-4e1d-8ac1-77ba9cb53277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-fe110470fda1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train-test split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfulltrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfulltrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfulltrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muserCol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m     n_train, n_test = _validate_shuffle_split(\n\u001b[1;32m   2421\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtype_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtype_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3345\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3346\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3347\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3349\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         return self.reduction(\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"len\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         ).compute()\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_keys__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0mpostcomputes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dask/threaded.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dsk, result, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mget_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_thread_get_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mpack_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpack_exception\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dask/local.py\u001b[0m in \u001b[0;36mget_async\u001b[0;34m(apply_async, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0;31m# Main loop, wait on tasks to finish, insert new ones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"waiting\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ready\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"running\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                     \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/dask/local.py\u001b[0m in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_k_item = 20000\n",
        "wu_size = 200\n",
        "max_item = wu_size\n",
        "\n",
        "top_items = ratings.groupby(itemCol).count().sort_values(by=userCol, ascending=False).head(top_k_item).index"
      ],
      "metadata": {
        "id": "iejmAYEB4kjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class  model"
      ],
      "metadata": {
        "id": "HIfY6FeqKqQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeTrachker():\n",
        "\t\"\"\"Tracking runing time\"\"\"\n",
        "\tdef start(self):\n",
        "\t\tself.start_time = time.time()\n",
        "\n",
        "\tdef check(self, des = ''):\n",
        "\t\ttry:\n",
        "\t\t\tif self.check_time is None:\n",
        "\t\t\t\tself.check_time = self.start_time\n",
        "\t\texcept:\n",
        "\t\t\tself.check_time = self.start_time\n",
        "\t\tself.end_time = time.time()\n",
        "\t\tdur = self.end_time - self.check_time\n",
        "\t\tprint(des + \" duration: \", dur)\n",
        "\n",
        "\t\tself.check_time = time.time()\n",
        "\t\treturn dur\n",
        "\n",
        "\tdef stop(self, des = ''):\n",
        "\t\tself.end_time = time.time()\n",
        "\t\tdur = self.end_time - self.start_time\n",
        "\t\tprint(des + \" duration: \", dur)\n",
        "\t\tself.start_time = time.time()\n",
        "\t\tself.check_time = None\n",
        "\t\treturn dur\n",
        "\n",
        "timer = TimeTrachker()"
      ],
      "metadata": {
        "id": "mvnW0Fd52VkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_interaction_set(interaction, max_item = None, top_k_item = None):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        interaction: df[userCol, itemCol, y]: dữ liệu đầu vào\n",
        "        max_item: int: item num limit\n",
        "    Output:\n",
        "        df, itemCol: list, y: list, itemCol_str: string, userCol as index\n",
        "        list item sắp xếp theo giảm dần độ lớn rating\n",
        "    \"\"\"\n",
        "    items = ratings.groupby(itemCol).count().sort_values(by=userCol, ascending=False)\n",
        "    if top_k_item is not None:\n",
        "        top_items = items.head(top_k_item).index\n",
        "        interaction = interaction[interaction[itemCol].isin(top_items)]\n",
        "    else:\n",
        "        top_items = items.index\n",
        "\n",
        "    # Sắp xếp item theo thứ tự giảm dần rating (về sau cắt padding sẽ ưu tiên giữ lại item có rating cao)\n",
        "    rindex = interaction.groupby(userCol)[\"y\"].transform(lambda grp: grp.sort_values(ascending=False).index)\n",
        "    interaction = interaction.reindex(rindex)\n",
        "    \n",
        "    # Chuyển thành warm-up set theo từng user\n",
        "    interaction = interaction.groupby(\"userId\").agg({itemCol:list, \"y\":list})\n",
        "\n",
        "    # Giới hạn độ dài warm_up size\n",
        "    if max_item is not None:\n",
        "        interaction[itemCol] = interaction[itemCol].apply(lambda x: x[0:max_item])\n",
        "        interaction[\"y\"] = interaction[\"y\"].apply(lambda x: x[0:max_item])\n",
        "\n",
        "    return interaction, top_items"
      ],
      "metadata": {
        "id": "Lb2Hx2rlWzmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bulding model\n",
        "class Efficient_Rec(tf.keras.Model):\n",
        "    def __init__(self, encoder, wu_size= 200, use_tf_function=False):\n",
        "        super().__init__()\n",
        "        self.use_tf_function = use_tf_function\n",
        "        self.encoder = encoder\n",
        "        self.wu_size = wu_size\n",
        "\n",
        "    # from v2.2: chỉ groupby, không padding\n",
        "    @staticmethod\n",
        "    def get_interaction_set(interaction, max_item = None, top_k_item = None):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            interaction: df[userCol, itemCol, y]: dữ liệu đầu vào\n",
        "            max_item: int: item num limit\n",
        "        Output:\n",
        "            df, itemCol: list, y: list, itemCol_str: string, userCol as index\n",
        "            list item sắp xếp theo giảm dần độ lớn rating\n",
        "        \"\"\"\n",
        "        items = ratings.groupby(itemCol).count().sort_values(by=userCol, ascending=False)\n",
        "        if top_k_item is not None:\n",
        "            top_items = items.head(top_k_item).index\n",
        "            interaction = interaction[interaction[itemCol].isin(top_items)]\n",
        "        else:\n",
        "            top_items = items.index\n",
        "\n",
        "        # Sắp xếp item theo thứ tự giảm dần rating (về sau cắt padding sẽ ưu tiên giữ lại item có rating cao)\n",
        "        rindex = interaction.groupby(userCol)[\"y\"].transform(lambda grp: grp.sort_values(ascending=False).index)\n",
        "        interaction = interaction.reindex(rindex)\n",
        "        \n",
        "        # Chuyển thành warm-up set theo từng user\n",
        "        interaction = interaction.groupby(\"userId\").agg({itemCol:list, \"y\":list})\n",
        "\n",
        "        # Giới hạn độ dài warm_up size\n",
        "        if max_item is not None:\n",
        "            interaction[itemCol] = interaction[itemCol].apply(lambda x: x[0:max_item])\n",
        "            interaction[\"y\"] = interaction[\"y\"].apply(lambda x: x[0:max_item])\n",
        "\n",
        "        return interaction, top_items\n",
        "\n",
        "    def _preprocess(self, inputs, padding_size = 100):\n",
        "        \"\"\"\n",
        "        Padding về wu_size và mask_size, convert list of items => string of items\n",
        "        batch_inputs: df: itemStr, y\"\"\"\n",
        "\n",
        "        def padding_list(list_item, wu_size, value=0, is_padding=True):\n",
        "            series_item1 = list_item[0:wu_size]\n",
        "            if is_padding:\n",
        "                series_item1 = series_item1+[value]*(wu_size-len(series_item1))\n",
        "            return series_item1\n",
        "\n",
        "        items_list, ratings_list = inputs\n",
        "\n",
        "        items   = items_list.apply(lambda x: ' '.join(list([str(i) for i in x])))\n",
        "        ratings =   np.stack( ratings_list.apply(lambda x: padding_list( x, padding_size  ) ) )\n",
        "\n",
        "        return items, ratings\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def get_top_cluster(scores, interaction_list):\n",
        "        interaction_list_ = interaction_list.copy()\n",
        "        idx = np.argsort(-scores.transpose(),axis=0)[:cluster_num]\n",
        "        interaction_list_[\"clusters\"] = [list(i) for i in idx.transpose()]\n",
        "        interaction_list_[\"scores\"] = [ list(scores[i][ind]) for i, ind in enumerate(idx.transpose()) ]\n",
        "        return interaction_list_\n",
        "\n",
        "    def minibatch_clustering(self, interaction_list, batch_size= 512):\n",
        "        chunks = [interaction_list[i:i+batch_size] for i in range(0,interaction_list.shape[0],batch_size)]\n",
        "        preds = []\n",
        "        for chunk in chunks:\n",
        "            pred = self.encoder(self._preprocess( [chunk[itemCol], chunk[\"y\"]], padding_size = wu_size )).numpy()\n",
        "            preds.append( pred )\n",
        "\n",
        "        return np.concatenate(preds)\n",
        "\n",
        "    @staticmethod\n",
        "    def chunk_explode(ratings, batch_size = 1024**2):\n",
        "        chunks = [ratings[i:i+batch_size] for i in range(0,ratings.shape[0],batch_size)]\n",
        "        explodes = []\n",
        "\n",
        "        # Todo: convert for loop to parallel\n",
        "        def chunk_process(chunk):\n",
        "            explode = chunk.explode([\"clusters\", \"scores\"])\n",
        "            explode[\"contribute_score\"] = explode[\"scores\"].astype(\"float64\")*explode[\"y\"]\n",
        "            explode = explode.groupby([\"clusters\", \"movieId\"]).agg({\n",
        "                \"contribute_score\": [\"mean\", \"count\"]\n",
        "            }).reset_index()\n",
        "            explode.columns = [\"clusters\", \"movieId\", \"mean\", \"count\"]\n",
        "\n",
        "            return explode\n",
        "\n",
        "        explodes = Parallel(n_jobs = -1, verbose = 1)(\n",
        "                    delayed(chunk_process)(chunk) for chunk in tqdm(chunks))\n",
        "        # combine results\n",
        "        gr = pd.concat(explodes, axis = 0)\n",
        "        gr[\"product\"] = gr[\"mean\"]*gr[\"count\"]\n",
        "        gr = gr.groupby([\"clusters\", \"movieId\"]).sum().reset_index()\n",
        "        gr[\"contribute_score\"] = gr[\"product\"]/gr[\"count\"]\n",
        "        gr = gr[[\"clusters\", \"movieId\", \"contribute_score\"]]\n",
        "        return gr\n",
        "\n",
        "    def get_shortlist(self, ratings, interaction_list= None, limit = 500, cluster_num = 5, batch_size=512):\n",
        "        timer.start()\n",
        "        if interaction_list is None:\n",
        "            interaction_list_, _ = self.get_interaction_set(  ratings, max_item = max_item, top_k_item = top_k_item  )\n",
        "        else:\n",
        "            interaction_list_ = interaction_list.copy()\n",
        "\n",
        "        scores = self.minibatch_clustering(interaction_list_, batch_size=batch_size)\n",
        "\n",
        "        # Limit number of cluster for each user\n",
        "        interaction_list_ = self.get_top_cluster(scores, interaction_list_)\n",
        "        timer.check(des = \"Get cluster\")\n",
        "\n",
        "        # Get shortlist for each cluster\n",
        "        ratings_ = ratings.copy().set_index(\"userId\")\n",
        "        ratings_ = ratings_[ratings_[\"y\"]>0]\n",
        "        ratings_ = ratings_.join(interaction_list_, rsuffix=\"_l\", how = \"inner\")\n",
        "        timer.check(des = \"Join interaction\")\n",
        "        ratings_ = self.chunk_explode(ratings_, batch_size= 1024*200)\n",
        "        timer.check(des = \"Chunk explode\")\n",
        "\n",
        "        ratings_[\"rank\"] = ratings_.groupby(\"clusters\")[\"contribute_score\"].rank(method='first', ascending=False)\n",
        "        ratings_ = ratings_[(ratings_[\"rank\"] <= limit)&(ratings_[\"contribute_score\"]>0)]\n",
        "        timer.check(des = \"Chunk explode\")\n",
        "\n",
        "        self.shortlist = ratings_\n",
        "\n",
        "        timer.stop(des = \"Total\")\n",
        "\n",
        "    @staticmethod\n",
        "    def left_anti_user_item_join( left, right ):\n",
        "        \"\"\"Fast left anijoin 2 dataframe by one column\"\"\"\n",
        "        wu_key = left[userCol].astype('str')+\"&\"+left[itemCol].astype('str')\n",
        "        blacklist = right[userCol].astype('str')+\"&\"+right[itemCol].astype('str')\n",
        "\n",
        "        key_diff = set(wu_key).difference(blacklist)\n",
        "        where_diff = wu_key.isin(key_diff)\n",
        "        output = left[where_diff]\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def batch_get_recommend(self, warm_up= None, historical_ratings = None, top_k = 10, is_remove_interacted = True, batch_size=1024):\n",
        "        timer.start()\n",
        "        if warm_up is None:\n",
        "            warm_up, _ = self.get_interaction_set( \n",
        "                         historical_ratings\n",
        "                        , max_item = max_item\n",
        "                        , top_k_item = top_k_item )\n",
        "\n",
        "        scores = self.minibatch_clustering(warm_up, batch_size=512)\n",
        "        wu = self.get_top_cluster( scores, warm_up)\n",
        "\n",
        "        wu = wu.explode([\"clusters\", \"scores\"]).reset_index()[[userCol, \"clusters\", \"scores\"]]\n",
        "        timer.check(des = \"Get cluster for user\")\n",
        "        \n",
        "        user_num = wu[userCol].drop_duplicates().shape[0]\n",
        "\n",
        "        chunks = [wu[wu[userCol].isin(range(i,i+batch_size))] \n",
        "                  for i in range(0,user_num,batch_size)]\n",
        "\n",
        "        his_chunks = [historical_ratings[historical_ratings[userCol].isin(range(i,i+batch_size))] \n",
        "                        for i in range(0,user_num,batch_size)]\n",
        "       \n",
        "        shortlist = self.shortlist\n",
        "\n",
        "        def batch_join_process(chunk, his_chunk):\n",
        "            wu = chunk.merge(shortlist, on=\"clusters\", how='inner')\n",
        "            wu[\"matched_score\"] = wu[\"scores\"]*wu[\"contribute_score\"]\n",
        "            wu = wu.groupby([userCol, itemCol]).agg({\"matched_score\":'mean'}).reset_index()\n",
        "            # using drop_duplicates for boost speed, may reduce accuracy.\n",
        "            # wu = wu.drop_duplicates(subset=[\"userId\", \"movieId\"])\n",
        "\n",
        "            if is_remove_interacted:\n",
        "                wu = self.left_anti_user_item_join( wu, historical_ratings )\n",
        "\n",
        "            wu[\"rank\"] = wu.groupby(userCol)[\"matched_score\"].rank(method='first', ascending=False)\n",
        "            wu = wu[wu[\"rank\"]<= top_k]\n",
        "\n",
        "            return wu \n",
        "        \n",
        "        timer.check(des = \"Prepare to join\")\n",
        "        wus = Parallel(n_jobs = -1, backend= 'threading', verbose = 1)(\n",
        "                    delayed(batch_join_process)(chunk, his_chunk) for chunk, his_chunk in tqdm(zip(chunks, his_chunks)))\n",
        "        \n",
        "        gc.collect()\n",
        "\n",
        "        timer.check(des = \"Join\")\n",
        "\n",
        "        output = pd.concat(wus, axis=0)\n",
        "\n",
        "        timer.stop(des = \"Total\")\n",
        "        return output\n",
        "\n",
        "    def release_cache(self):\n",
        "        self.interaction_list = None \n",
        "        gc.collect()"
      ],
      "metadata": {
        "id": "jJySyZdU4Iou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Xây dựng encoder model\n",
        "Encoder =  interaction embedding + user feature embedding<br> \n",
        "interaction embedding = sum( interaction embedding các item i)<br> \n",
        "interaction embedding item i = rating x (embedding id sản phẩm + embedding item feature)<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "Sbp1tH7yUeI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Vectorize (encode + padding) item list\n",
        "max_vocab_size = len(top_items) # nếu số item có <= top_k_item => lấy số lượng item max\n",
        "items_str = ' '.join([str(i) for i in top_items])\n",
        "itemStr = itemCol+\"_str\"\n",
        "\n",
        "vectorizer = layers.TextVectorization( max_tokens= top_k_item, split='whitespace', output_sequence_length= wu_size, name = 'vectorizer')\n",
        "vectorizer.adapt( [items_str] ) "
      ],
      "metadata": {
        "id": "QxNyuo9-WJJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Broadcasting_Multiply(tf.keras.layers.Layer):\n",
        "    \"\"\"Nhân 2 layers khác shape với nhau, trong đó:\n",
        "    inputs=[layer1, layer2]\n",
        "    layer1.shape = (None, n_item, n_feature)\n",
        "    layer2.shape = (None, n_item)\n",
        "    (Chú ý đúng thứ tự)\n",
        "    \"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, y = inputs\n",
        "        deno = tf.expand_dims(tf.cast(tf.math.count_nonzero(y, axis=1), tf.float32), -1)\n",
        "        #we add the extra dimension:\n",
        "        y = K.expand_dims(y, axis=-1)\n",
        "        #we replicate the elements\n",
        "        y = K.repeat_elements(y, rep=x.shape[2], axis=-1)\n",
        "\n",
        "        return x * y, deno"
      ],
      "metadata": {
        "id": "1U7EQG6zWpi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Xây dựng mạng\n",
        "embedding_size = 173\n",
        "reps_size = 132\n",
        "cluster_num = 43\n",
        "\n",
        "@tf.function\n",
        "def avg_layer(z):\n",
        "    t = K.sum(z[0], axis=1)/z[1]\n",
        "    t = tf.clip_by_value( t, -1, 1 )\n",
        "    t = tf.where(tf.math.is_nan(t), tf.zeros_like(t), t)\n",
        "    return t\n",
        "\n",
        "\n",
        "def interaction_embedding():\n",
        "\n",
        "    input_wi = layers.Input(shape=(1,), name='input_wi')\n",
        "    wi = vectorizer(input_wi)\n",
        "    wi = layers.Embedding(input_dim= max_vocab_size, output_dim= embedding_size, mask_zero= True, name='ei')(wi)\n",
        "    # wi = layers.Dense(embedding_size, activation='sigmoid', use_bias = False, name='di')(wi)\n",
        "    wi = layers.Dense(embedding_size, activation='relu', use_bias = False, name='di1')(wi)\n",
        "    wi = layers.Dense(embedding_size, activation='relu', use_bias = False, name='di2')(wi)\n",
        "    # wi = layers.Dense(embedding_size, activation='sigmoid', use_bias = False, name='di3')(wi)\n",
        "\n",
        "    wr = layers.Input(shape=(wu_size,), name='warm_up_ratings')\n",
        "\n",
        "    ireps = Broadcasting_Multiply(name='mul')([wi, wr])\n",
        "    uprofile = layers.Lambda(lambda z: avg_layer(z) )(ireps)\n",
        "\n",
        "    uprofile = layers.Dense( reps_size, activation='relu', name='du1')(uprofile)\n",
        "    uprofile = layers.Dense( reps_size, activation='relu', name='du2')(uprofile)\n",
        "    # uprofile = layers.Dense( reps_size, activation='relu', name='du3')(uprofile)\n",
        "    # uprofile = layers.BatchNormalization(name='norm')(uprofile)\n",
        "    uprofile = layers.LayerNormalization(name='norm')(uprofile)\n",
        "    # uprofile = layers.Dense( reps_size, activation='relu', name='du4')(uprofile)\n",
        "    uprofile = layers.Dense(cluster_num, activation='sigmoid', name='clustering')(uprofile)\n",
        "    \n",
        "    \n",
        "    model = tf.keras.Model(inputs= [input_wi, wr], outputs=[uprofile])\n",
        "    return model"
      ],
      "metadata": {
        "id": "--fbIsGLXjPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example of layer interaction embedding step by step\n",
        "# input_wi = [\"15 25 65 20 84\",  # 5 items\n",
        "#             \"51 54 45 21 24 83 81 76 74 75 72 48 29 38\",# 14 items\n",
        "#             \" \",] \n",
        "\n",
        "# tvectorizer = layers.TextVectorization( max_tokens= 17, split='whitespace', output_sequence_length= 10)\n",
        "# tvectorizer.adapt( input_wi ) \n",
        "\n",
        "# wi = tvectorizer(input_wi)\n",
        "# print(wi)\n",
        "# wi = layers.Embedding(input_dim= 17, output_dim= 4, mask_zero= True, name='ei')(wi)\n",
        "# print(wi)\n",
        "# wi = layers.Dense(3, activation='sigmoid',  use_bias = False, name='di')(wi)\n",
        "\n",
        "# wr = np.array([[0.5, 0.1, -0.5, 1, 0.25, 0, 0, 0, 0, 0], [0.25, 0.15, 0.5, 1, 0.25, 0.5, 0.1, -0.9, 0.4, -0.3], [0,0,0,0,0,0,0,0,0,0]])\n",
        "\n",
        "# ireps = Broadcasting_Multiply(name='mul')([wi, wr])\n",
        "# print(ireps)\n",
        "# uprofile = layers.Lambda(lambda z: avg_layer(z) )(ireps)\n",
        "# print(uprofile)\n",
        "\n",
        "# uprofile = layers.Dense( reps_size, activation='relu', name='du2')(uprofile)\n",
        "# uprofile = layers.LayerNormalization(name='norm')(uprofile)\n",
        "# uprofile = layers.Dense(5, activation='sigmoid', name='clustering')(uprofile)\n",
        "# print(uprofile)"
      ],
      "metadata": {
        "id": "a7EW-lg9Cfi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kiểm tra tham số\n",
        "# interaction_embedding().summary()"
      ],
      "metadata": {
        "id": "cJMxOoxzXvr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.utils.plot_model( interaction_embedding() ,show_shapes=True, show_dtype=True, show_layer_names=True )"
      ],
      "metadata": {
        "id": "PuAM8w01Xy93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate model results"
      ],
      "metadata": {
        "id": "BISfMUNM37ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_evaluate(model, movies, df):\n",
        "    dfu, ttop_items = get_interaction_set( df\n",
        "                    , max_item = max_item\n",
        "                    , top_k_item = top_k_item )\n",
        "    group_scores = model.encoder(model._preprocess( [dfu[itemCol], dfu[\"y\"]], padding_size = wu_size )).numpy()\n",
        "\n",
        "    print(\"SAMPLE INTERACTION EMBEDDING\")\n",
        "    print( np.max(group_scores), np.mean(group_scores), np.min(group_scores) )\n",
        "    print( group_scores[0:3] )\n",
        "\n",
        "    print(\"FEATURE PLOT\")\n",
        "    feature_plot( movies, df, group_scores)\n",
        "    \n",
        "    print(\"CLUSTER CHECKING\")\n",
        "    check_cluster(group_scores)\n",
        "    \n",
        "    print(\"SPECTROGRAM PLOT\")\n",
        "    plot_spectrogram(group_scores)\n",
        "\n",
        "def get_label(movies, df, is_encode = False):\n",
        "    movies[\"genres_list\"] = movies[\"genres\"].apply(lambda x: x.split(' '))\n",
        "    movie_genres = movies.explode(\"genres_list\")\n",
        "    gr = df.merge(movie_genres, on=\"movieId\").groupby([\"userId\", \"genres_list\"])[\"movieId\"].count().reset_index()\n",
        "    gr[\"rank\"] = gr.groupby(\"userId\")[\"movieId\"].rank(method='first', ascending=False)\n",
        "\n",
        "    labels = gr[gr[\"rank\"] ==1].set_index(\"userId\")\n",
        "    # labels[\"pred_max_ind\"] = np.argmax(group_scores, axis=1)\n",
        "\n",
        "    if is_encode:\n",
        "        label_enc = LabelEncoder()\n",
        "        labels[\"label\"] = label_enc.fit_transform(labels[\"genres_list\"])\n",
        "\n",
        "    return labels\n",
        "\n",
        "def feature_plot( movies, df, group_scores ):\n",
        "    tlabels = get_label(movies, df)\n",
        "\n",
        "    tsne = PCA(n_components=2, random_state=123)\n",
        "    # tsne = TSNE(n_components=2, random_state=123)\n",
        "    z = tsne.fit_transform(group_scores) \n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    df[\"y\"] = tlabels[\"genres_list\"]\n",
        "    df[\"comp-1\"] = z[:,0]\n",
        "    df[\"comp-2\"] = z[:,1]\n",
        "\n",
        "    plt.rcParams[\"figure.figsize\"] = (8,8)\n",
        "    sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
        "                    palette=\"Paired\" ,#sns.color_palette(\"hls\", 3),\n",
        "                    data=df)#.set(title=\"Iris data T-SNE projection\") \n",
        "    plt.show()\n",
        "\n",
        "def check_cluster(group_scores):\n",
        "    # Kiểm tra số user trong mỗi cụm có bị vón cục\n",
        "    ugs= np.argmax(group_scores, axis=1)\n",
        "    for i in range(50):\n",
        "        print(i,': ', np.sum(ugs==i) )\n",
        "\n",
        "def plot_spectrogram(group_scores):\n",
        "    # Sort theo user_group + draw sigmoid/softmax layer\n",
        "    plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "    k =100\n",
        "    a = group_scores\n",
        "    ind = np.argmax(group_scores, axis=1)\n",
        "    plt.imshow( a[np.argsort(ind)][0:k] )\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "83VIKSPGNrN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_plot(model, movies, df):\n",
        "    dfu, ttop_items = get_interaction_set( df\n",
        "                    , max_item = max_item\n",
        "                    , top_k_item = top_k_item )\n",
        "    group_scores = model.encoder(model._preprocess( [dfu[itemCol], dfu[\"y\"]], padding_size = wu_size )).numpy()\n",
        "    print(\"FEATURE PLOT\")\n",
        "    feature_plot( movies, df, group_scores)\n",
        "    "
      ],
      "metadata": {
        "id": "_cs4Ci70vrkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Warm start user"
      ],
      "metadata": {
        "id": "xfUOMI8subTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "u_train_from = 0\n",
        "u_train_to = u_train_from + 100000\n",
        "u_test = u_train_to + 5000\n",
        "\n",
        "def get_labeled_data(df):\n",
        "    interact_df, _ = get_interaction_set( \n",
        "                     df\n",
        "                    , max_item = max_item\n",
        "                    , top_k_item = top_k_item )\n",
        "    labels = get_label( movies, df, is_encode = True)\n",
        "    interact_df[\"label\"] = labels[\"label\"]\n",
        "    return interact_df[[\"movieId\",\"y\",\"label\"]]\n",
        "try:\n",
        "    train_warm\n",
        "except:\n",
        "    # if exists, do not rerun\n",
        "    train_warm =  get_labeled_data( train[(train[userCol]>u_train_from)&(train[userCol]<u_train_to)] )\n",
        "    train_warm[\"rating_num\"] = train_warm.apply(lambda x: len(x[\"y\"]), axis=1)\n",
        "    # pretrain with warm start user\n",
        "    train_warm = train_warm[train_warm[\"rating_num\"]>=100]\n",
        "\n",
        "train_warm.count()"
      ],
      "metadata": {
        "id": "iQtBh9xFunkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "DE0hPo7UFNkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# supervised constrastive "
      ],
      "metadata": {
        "id": "9wO4EX1_Z-h3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SupervisedContrastiveLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, temperature=1, name=None):\n",
        "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
        "        # Normalize feature vectors\n",
        "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
        "        # Compute logits\n",
        "        logits = tf.divide(\n",
        "            tf.matmul(\n",
        "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
        "            ),\n",
        "            self.temperature,\n",
        "        )\n",
        "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)"
      ],
      "metadata": {
        "id": "hQ-pwUZafD__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện training\n",
        "def _supervised_constrastive_train_step(self, inputs):\n",
        "    items_pd, ratings_pd, labels = inputs[\"movieId\"], inputs[\"y\"], inputs[\"label\"]\n",
        "    items, ratings = self._preprocess((items_pd, ratings_pd), wu_size)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Interaction embedding\n",
        "        vec = self.encoder([items, ratings])\n",
        "\n",
        "        average_loss = self.loss(labels, vec)\n",
        "\n",
        "    # Apply an optimization step\n",
        "    variables = self.trainable_variables \n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    \n",
        "    # Gradient clipping\n",
        "    # gradients = [None if gradient is None else tf.clip_by_value(gradient, -0.1, 0.1)\n",
        "    #              for gradient in gradients]\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "\n",
        "Efficient_Rec._supervised_constrastive_train_step = _supervised_constrastive_train_step"
      ],
      "metadata": {
        "id": "TbltflgfUmXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thực hiện minibatch training\n",
        "def _spv_constrastive_train_minibatch_step(self, inputs, batch_size):\n",
        "    df = inputs.copy()\n",
        "    chunks = [df[i:i+batch_size] for i in range(0,df.shape[0],batch_size)]\n",
        "    losses = []\n",
        "    for chunk in chunks:\n",
        "        loss = self._supervised_constrastive_train_step(chunk)\n",
        "        losses.append(loss[\"batch_loss\"].numpy())\n",
        "        print(loss)\n",
        "        gc.collect()\n",
        "    return np.mean(losses)\n",
        "\n",
        "Efficient_Rec._spv_constrastive_train_minibatch_step = _spv_constrastive_train_minibatch_step"
      ],
      "metadata": {
        "id": "qSr05iptaPha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile model\n",
        "model = Efficient_Rec( encoder = interaction_embedding(), \n",
        "                      wu_size = wu_size,\n",
        "                      use_tf_function=False)\n",
        "model.compile(\n",
        "    optimizer=tf.optimizers.Adam(learning_rate = 0.001),\n",
        "    loss= TripletSemiHardLoss() # SupervisedContrastiveLoss(temperature = 0.05),\n",
        ")"
      ],
      "metadata": {
        "id": "uBCoJlJMdB7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "epochs= 1\n",
        "test_set = train[(train[userCol]>10000)&(train[userCol]<12500)]\n",
        "model_plot(model, movies, test_set )\n",
        "for n in range(epochs):\n",
        "  print(n, \"/\", epochs, \": \", model._spv_constrastive_train_minibatch_step(train_warm.sample(frac=0.1), batch_size=512))\n",
        "  model_plot(model, movies, test_set )\n",
        "  gc.collect()"
      ],
      "metadata": {
        "id": "VoepslyXhaQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_evaluate(model, movies, \n",
        "#                warm_up_mask[(warm_up_mask[userCol]>u_train_to)&(warm_up_mask[userCol]<(u_train_to+5000))])"
      ],
      "metadata": {
        "id": "ZZUCY20TSjFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pick item pipeline"
      ],
      "metadata": {
        "id": "CgfpZRoGF1KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "# START\n",
        "# model.get_shortlist( train, limit = 500, cluster_num = 5)\n",
        "\n",
        "# Pick shortlist base on warm start user only\n",
        "# Large cluster num, small limit top item\n",
        "model.get_shortlist( \n",
        "    ratings = train[train[userCol].isin(train_warm.index)],\n",
        "    interaction_list = train_warm,\n",
        "    limit = 250, \n",
        "    cluster_num = 10\n",
        ")\n"
      ],
      "metadata": {
        "id": "fQtrddWcEaoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "0EvrD1tOVkXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.shortlist"
      ],
      "metadata": {
        "id": "pO4O1LwBXrOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "# interaction_list = train_warm[train_warm.index<=3000]\n",
        "top_k = 50\n",
        "is_remove_interacted = True\n",
        "\n",
        "y_pred = model.batch_get_recommend( \n",
        "        warm_up = train_warm, \n",
        "        historical_ratings= train, \n",
        "        top_k = top_k, is_remove_interacted = False, batch_size=1024*4\n",
        "    )"
      ],
      "metadata": {
        "id": "a1MUU9bdHQAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_warm"
      ],
      "metadata": {
        "id": "7yeBK3lxHfDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred[\"rating\"] = y_pred[\"matched_score\"].astype(\"float\")*2.5+2.5"
      ],
      "metadata": {
        "id": "rpEi4FIaJw04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "c8yrKhc16vyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = test[test[\"y\"]>0]"
      ],
      "metadata": {
        "id": "2dGbwAJOur5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred.dtypes"
      ],
      "metadata": {
        "id": "po-ftpJd6PRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "eval_map = map_at_k(y_true, y_pred, col_user = userCol, col_item = itemCol ,col_prediction='rating', col_rating=\"rating\", k=top_k)\n",
        "eval_ndcg = ndcg_at_k(y_true, y_pred, col_user = userCol, col_item = itemCol ,col_prediction='rating', col_rating=\"rating\", k=top_k)\n",
        "eval_precision = precision_at_k(y_true, y_pred, col_user = userCol, col_item = itemCol ,col_prediction='rating', col_rating=\"rating\", k=top_k)\n",
        "eval_recall = recall_at_k(y_true, y_pred, col_user = userCol, col_item = itemCol, col_prediction='rating', col_rating=\"rating\", k=top_k)\n",
        "\n",
        "print('K = %f' % top_k)\n",
        "print(\n",
        "    \"MAP:\\t%f\" % eval_map,\n",
        "      \"NDCG:\\t%f\" % eval_ndcg,\n",
        "      \"Precision@K:\\t%f\" % eval_precision,\n",
        "      \"Recall@K:\\t%f\" % eval_recall, sep='\\n')"
      ],
      "metadata": {
        "id": "yZKN6iNvJQFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true2 = test\n",
        "y_true2[\"is_fav\"] = y_true2[\"rating\"].apply(lambda x: 1 if x>2.5 else 0)"
      ],
      "metadata": {
        "id": "H16_tFTPLXa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred2 = y_pred\n",
        "y_pred2[\"is_rec\"] = 1"
      ],
      "metadata": {
        "id": "DNfK3ow2Lrtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rs2(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    y_true: dataframe: user_id, item_id, is_fav (1 true, 0 false)\n",
        "    y_pred: dataframe: user_id, item_id\n",
        "    return:\n",
        "    precision@k, recall@k\n",
        "    \"\"\"\n",
        "    y_pred2 = y_pred.copy()\n",
        "    y_pred2[\"is_rec\"] = 1\n",
        "\n",
        "    y_true2 = y_true.copy()\n",
        "\n",
        "    total1 = y_true2.merge(y_pred2, on=[userCol, itemCol], how = 'left')\n",
        "\n",
        "    total1[\"rec_fav\"] = total1[\"is_rec\"].fillna(0) * total1[\"is_fav\"]\n",
        "\n",
        "    # Precision\n",
        "    p = total1[\"rec_fav\"].sum() / total1[\"is_rec\"].sum()\n",
        "\n",
        "    # Recall\n",
        "    r = total1[\"rec_fav\"].sum() / total1[\"is_fav\"].sum()\n",
        "\n",
        "    # print(\"Precision : \", p)\n",
        "    # print(\"Recall : \", r)\n",
        "    return p, r"
      ],
      "metadata": {
        "id": "MWcFUMZ0KoSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall = evaluate_rs2(y_true2, y_pred)"
      ],
      "metadata": {
        "id": "D9AtAbTSOVbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Precision :\", precision)"
      ],
      "metadata": {
        "id": "N2xZOMMPPcsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END HERE"
      ],
      "metadata": {
        "id": "IYIY1sccKoqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1/0"
      ],
      "metadata": {
        "id": "z8mwDaKFKp4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rs(y_true, y_pred, is_return_df = False):\n",
        "    \"\"\"\n",
        "    y_true: dataframe: user_id, item_id, y (rating normailised), only favorite item\n",
        "    y_pred: dataframe: user_id, item_id (just top k item)\n",
        "    return:\n",
        "    precision@k, recall@k\n",
        "    \"\"\"\n",
        "    total1 = y_true.merge(y_pred, on=[userCol, itemCol], how = 'outer', suffixes=('_t', '_p'))\n",
        "    total1['is_pt'] = total1.apply(lambda x: 0 if (np.isnan(x[\"y\"]) or np.isnan(x[\"rank\"]) ) else 1,axis=1)\n",
        "    total = total1.groupby(userCol).agg({\n",
        "        \"y\":'count',\n",
        "        \"rank\":'count',\n",
        "        \"is_pt\": 'sum'\n",
        "        })\n",
        "    total.columns = [\"true_num\", \"predict_num\", \"pred_true_num\"]\n",
        "    total[\"macro_p\"] = total[\"pred_true_num\"]/total[\"predict_num\"]\n",
        "    total[\"macro_r\"] = total[\"pred_true_num\"]/total[\"true_num\"]\n",
        "\n",
        "    total = total[total[\"predict_num\"]>0]\n",
        "\n",
        "    macro_p = total[total[\"predict_num\"]>0][\"macro_p\"].mean()\n",
        "    macro_r = total[total[\"true_num\"]>0][\"macro_r\"].mean()\n",
        "\n",
        "    micro_p = total[\"pred_true_num\"].sum()/total[\"predict_num\"].sum()\n",
        "    micro_r = total[\"pred_true_num\"].sum()/total[\"true_num\"].sum()\n",
        "\n",
        "    print(\"macro_p: \", macro_p, \"; macro_r :\", macro_r)\n",
        "    print(\"micro_p: \", micro_p, \"; micro_r :\", micro_r)\n",
        "\n",
        "    if is_return_df:\n",
        "        return (macro_p, macro_r, micro_p, micro_r), total\n",
        "\n",
        "    return macro_p, macro_r, micro_p, micro_r"
      ],
      "metadata": {
        "id": "QPPnAXS9vBhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "eval_metrics, eval_df = evaluate_rs(y_true, y_pred, is_return_df= True)"
      ],
      "metadata": {
        "id": "i91nkjJq793s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_df[eval_df['pred_true_num']>0]"
      ],
      "metadata": {
        "id": "4vVTlZbJ-AbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "macro_p, macro_r, micro_p, micro_r= evaluate_rs(y_true, y_pred)"
      ],
      "metadata": {
        "id": "HM00AWaQ1H1g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "v2.2_ML20M_sequence.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "17fHxkHoAQq6"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}